# Containerization Concepts

## What is Containerization?

Containerization packages an ML model and its dependencies (e.g., Python libraries, frameworks like TensorFlow or PyTorch, and custom scripts) into a **self-contained, portable unit**.

This ensures it can run **consistently** across different environments (local dev, ECS, EKS, etc.).

## Why Containerize ML Models?

| Benefit       | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| **Portability** | Same image can run on local machine, ECS, or EKS with minimal changes     |
| **Isolation**   | Each container runs independently with its own dependencies               |
| **Scalability** | ECS/EKS can auto-scale based on traffic, CPU, memory                      |
| **Standardization** | Docker image format simplifies sharing across teams and environments   |

## AWS Container Services

### Amazon Elastic Container Service (ECS)
- **Launch Types**:
  - **EC2 Launch Type**: You manage the EC2 infrastructure
  - **Fargate Launch Type**: Serverless, just define CPU/memory needs
- **Use Cases**:
  - Host custom inference servers
  - Run batch jobs (e.g., preprocessing, feature engineering)
- **Integration**: Can be used with SageMaker or standalone

### Amazon Elastic Kubernetes Service (EKS)
- Managed Kubernetes service on AWS
- Ideal for advanced ML pipelines using tools like **Kubeflow**, **Spark on K8s**
- **Fargate on EKS**: Lets you run pods without managing worker nodes

| Pros                                | Cons                                                  |
|-------------------------------------|-------------------------------------------------------|
| Integrates with existing Kubernetes tools (e.g. Kubeflow) | Steep learning curve                                 |
| Large open-source ecosystem         | Overkill for simple container tasks                  |

### Amazon Elastic Container Registry (ECR)
A fully managed Docker container registry to store, version, and secure container images.

- **IAM-secured access**
- **Tagging** (e.g., `model:v1`, `model:latest`)
- **Scanning for vulnerabilities**

**Typical Workflow**:
1. Build image:  
   `docker build -t my-model:v1 .`
2. Authenticate to ECR:  
   `aws ecr get-login-password | docker login ...`
3. Push to ECR:  
   `docker push <account_id>.dkr.ecr.<region>.amazonaws.com/my-model:v1`

## Sample Dockerfile for ML Inference

```dockerfile
FROM python:3.9
RUN pip install boto3 sagemaker scikit-learn
COPY inference.py /app/inference.py
ENTRYPOINT [ "python", "/app/inference.py" ]
```

- **Local Test**:  
  `docker run -p 8080:8080 my-model:v1`

- **Security Tips**:
  - Avoid hardcoding secrets in Docker images
  - Use AWS Secrets Manager or ECS/EKS environment variables
  - Regularly update base images

## Managing Images in ECR

| Task              | Command |
|-------------------|---------|
| Create repo       | `aws ecr create-repository --repository-name <my-repo>` |
| Authenticate      | `aws ecr get-login-password ... | docker login ...` |
| Push image        | `docker push <account_id>.dkr.ecr.../<my-repo>:v1` |
| Tag image         | `docker tag my-model:latest my-model:v2` |
| Enable scans      | ECR settings allow automated scanning for vulnerabilities |
| Cleanup           | Define ECR lifecycle policies to manage old versions     |

## Container-Based Deployment Patterns

| Pattern              | Description |
|----------------------|-------------|
| **Single-Model Container** | One model per container; easier management, higher cost at scale |
| **Multi-Model Container**  | Multiple models per container; routes requests internally |
| **Sidecar Container**      | Run support containers (e.g., logging, monitoring) alongside model container |

# Choosing Appropriate Containers

Containerization in machine learning ensures **reliable**, **consistent**, and **cost-efficient** model deployment environments. Containers streamline deployment, scaling, and orchestration across various environments.

## Selecting Container Types

### Pre-built Containers

SageMaker provides **pre-configured containers** for popular ML frameworks:

- TensorFlow
- PyTorch
- Scikit-learn
- XGBoost

These containers come with optimized libraries and dependencies already installed.

**When to Use Pre-built Containers:**
- Using standard model architectures
- Rapid prototyping (e.g., hackathons, demos)
- Tight deadlines where **speed of deployment** is key

**Advantages:**
- Faster deployment
- Lower complexity in setup
- Reliable compatibility with AWS features

### Customized Containers

Custom containers allow **full control** over the runtime environment:

- Add your own dependencies, scripts, and configurations
- Fit niche frameworks or regulatory environments

**When to Use Custom Containers:**
- Using private or proprietary pre-trained models
- Non-standard or niche ML frameworks
- Compliance and security concerns (e.g., financial systems under BSP regulations)

**Advantages:**
- Full flexibility
- Tailored security settings
- Exact dependency management

## Containerization for Edge Devices with SageMaker Neo

**SageMaker Neo** allows optimization and compilation of ML models for **edge deployment**.

### Benefits:
- **Lower latency** for inference
- **Reduced memory usage**
- **Improved efficiency** on limited-resource devices

### How to Deploy with SageMaker Neo

1. **Train the Model**
   - Use regular SageMaker training jobs

2. **Compile with Neo**
   - Optimize for target hardware (e.g., ARM, Raspberry Pi)

3. **Deploy with Edge Manager**
   - Use SageMaker Edge Manager to securely deploy and manage the model at the edge


## Multi-Model vs Multi-Container Deployment

### Multi-Model Endpoint (MME)
- **Use Case**: Many models, rarely accessed
- **Advantages**: Cost-effective (shared resources), simple to manage
- **Challenges**: Cold start latency, shared performance limits
- **Example**: Restaurant chain in Manila with multiple menu models

### Multi-Container Endpoint
- **Use Case**: Models with different hardware/resource needs
- **Advantages**: Full isolation, adaptability, scalability
- **Challenges**: More costly and complex
- **Example**: Ayala Corp uses GPU for vision, CPU for stats models

| Strategy             | Cost         | Performance     | Best Fit                                        |
|----------------------|--------------|------------------|--------------------------------------------------|
| Multi-Model Endpoint | Low          | Varies (cold starts) | Infrequent usage, lightweight models             |
| Multi-Container      | High         | High              | High-demand models with varied requirements      |
