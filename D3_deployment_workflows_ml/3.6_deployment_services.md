# SERVICES FOR DEPLOYMENT

## Amazon SageMaker

Amazon SageMaker is AWS’s all-in-one machine learning platform. It helps with:

- Data labeling
- Model training using built-in algorithms
- Deployment through managed endpoints

With **SageMaker Model Hosting**, models can be deployed as REST endpoints, with built-in:

- Hardware provisioning
- Auto-scaling
- Load management

### Why Use SageMaker for Deployment?

- **Innate Scaling**  
  Automatically adjusts capacity based on request volume.

- **Reduced Ops Overhead**  
  Handles infrastructure, networking, and updates.

- **Tracking & Monitoring**  
  Integrates with **Amazon CloudWatch** for metrics like latency and error rates.

- **Model Versioning & Rollbacks**  
  Use the **SageMaker Model Registry** to manage and revert versions.

### SageMaker Deployment Options

- **Real-Time Endpoints**  
  For fast, low-latency, continuous predictions.

- **Batch Transform**  
  For scheduled or bulk predictions (e.g., hourly jobs).

- **Asynchronous Inference**  
  For long-running inference tasks with deferred response.

- **Serverless Inference**  
  Zero-capacity during idle time; scales automatically when requests increase.

## Other AWS Services for Model Deployment

### AWS Lambda

- **Best for**: Lightweight, event-driven tasks or pre-/post-processing.
- **Pros**:
  - Serverless (no maintenance)
  - Pay-per-call model
- **Cons**:
  - Not suitable for large models or high throughput

Deploying the model within AWS Lambda as a function and exposing it through an API Gateway endpoint is ideal for lightweight, serverless, real-time inference. Lambda’s automatic scaling and pay-per-use model align well with unpredictable traffic patterns and the need for cost efficiency.

While AWS Lambda is excellent for serverless applications, it may not be the best choice for a model if it requires continuous, low-latency processing or needs to handle very high throughput. Lambda is better suited for lightweight, event-driven tasks rather than long-running, complex inference jobs.

### ECS / EKS (Elastic Container & Kubernetes Services)

- **Best for**: Custom container setups or microservices architectures
- **Pros**:
  - Full control over infrastructure
  - Works well for advanced ML environments
- **Cons**:
  - Higher complexity and manual management

### AWS Elastic Beanstalk

- **Best for**: Web apps that include ML models
- **Pros**:
  - Easy setup and deployment
  - Automatically provisions EC2, load balancers, etc.
- **Cons**:
  - Not tailored for ML workflows

### AWS App Runner

- **Best for**: Simple web-based predictions using containers
- **Pros**:
  - Fully managed, auto-scaled container service
- **Cons**:
  - Lacks ML-specific features like monitoring and model registry

## SageMaker Endpoints: Hosting & Features

SageMaker endpoints allow models to be deployed as a **REST API**.

### Endpoint Configuration Includes:
- Model path
- Instance type
- Initial scaling settings
- Optional encryption

### Key Features:
- **Autoscaling**: Based on requests or latency thresholds
- **Multi-Model Endpoints**: Host multiple models on one endpoint
- **Traffic Shifting**: Smooth rollouts by routing partial traffic
- **Secure Access**: IAM, VPC, and SSL integration


### SageMaker autoscaling
Amazon SageMaker supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.

Enabling Auto Scaling allows the endpoint to dynamically adjust the number of instances based on actual traffic. By targeting instance utilization, the deployment can automatically scale out during peak times and scale in during low demand, improving both response time and throughput without over-provisioning. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.


### Multi-model endpoint

Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models.

Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances.

Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints.

## AWS Lambda for Serverless Inference

- **Use case**: Triggered predictions from events like API calls or DB changes
- **Advantages**:
  - Event-driven and cost-efficient
  - Fully serverless and automatically scaled
- **Limitations**:
  - Unsuitable for large models or heavy workloads
  - Not ideal for batch or real-time high-volume inference

### AWS Lambda for Model Serving

For infrequent inference tasks, Lambda is more cost-efficient than running SageMaker 24/7.

### Architecture:

- Lambda function holds the model or pulls it from **S3**, **EFS**, or **ECR**
- Triggers: **S3 events**, **API Gateway**, **EventBridge**
- Performs inference, then terminates

### ⚠️ Trade-Offs:

| Limitation          | Notes                                         |
|---------------------|-----------------------------------------------|
| Cold Starts         | Delay during first load, especially for large models |
| Memory Limits       | Max 10GB RAM – may be too low for some models |
| Stateless           | Unsuitable for stateful or GPU-heavy workloads |

## Comparative Analysis

| Deployment Option | Scalability                  | Cost                     | Control                       | Complexity         |
|------------------|------------------------------|--------------------------|-------------------------------|--------------------|
| **SageMaker**     | Auto-scales by traffic       | Pay-as-you-go            | Limited (managed service)     | Low                |
| **Lambda**        | Event-driven scaling         | Low for small workloads  | Minimal                       | Low                |
| **ECS/EKS**       | Highly scalable, configurable| Potentially higher       | Full control over containers  | High               |

## Summary

- **SageMaker Endpoints**: Best for **managed, scalable**, and **real-time** ML model hosting.
- **AWS Lambda**: Ideal for **small, serverless, event-triggered** inference jobs.
- **ECS/EKS**: Suitable for **custom container setups** and **advanced orchestration** needs.


