# Serving ML Models in Real-Time and Batch Modes

## Real-Time Inference with SageMaker

### What is Real-Time Inference?

Real-time inference delivers **immediate predictions** upon request. It's crucial for low-latency systems like:

- Fraud detection in finance
- Spam or email filtering
- Interactive applications (chatbots, personalized ads)

### Steps to Implement Real-Time Inference in SageMaker

1. **Prepare Your Model**  
   - Train your model or use pre-trained models (e.g., TensorFlow, PyTorch)  
   - Save artifacts in Amazon S3  

2. **Build a Model Object**  
   - Package the model into a Docker container  
   - Link the container with model artifacts in S3  

3. **Set Up an Endpoint**  
   - Choose instance type (e.g., `ml.m5.large`, `ml.c5.xlarge`)  
   - Optionally enable autoscaling for load-based instance adjustment  

4. **Deploy the Endpoint**  
   - Once deployed, SageMaker provides an HTTPS endpoint  
   - Send a `POST` request with input data  
   - Receive predictions in real time  

### Best Practices for Real-Time Inference

- **Autoscaling**: Use SageMaker's automatic scaling features
- **Monitoring**: Track metrics with Amazon CloudWatch
- **Multi-Model Endpoints**: Serve multiple models via a single endpoint
- **Deployment Tactics**: Use canary or blue/green strategies for safe rollouts
- **Security**: Secure endpoints using IAM, VPC, and SageMaker PrivateLink

## Batch Inference with SageMaker

### What is Batch Inference?

Batch inference (a.k.a. **Batch Transform**) is optimized for **large-scale, non-time-sensitive** processing.

Used for:
- Periodic reports
- Trend analysis
- Offline data scoring

### Why Use Batch Transform?

- **High-Volume Processing**: Efficient with terabytes of data
- **No Persistent Endpoint**: No idle compute cost
- **Offline Integration**: Ideal for asynchronous workloads

### Steps to Implement Batch Transform in SageMaker

1. **Upload Input Data**: Place raw input (e.g., CSV) in Amazon S3  
2. **Create a Batch Transform Job**:
   - Reference model artifacts (from SageMaker or elsewhere)
   - Specify input and output S3 paths  
3. **Choose Compute Resources**:
   - Pick instance types suited for memory and CPU/GPU needs  
4. **Run the Job**:
   - SageMaker provisions resources, processes batches, and stores results in S3  
5. **Tear Down Automatically**:
   - Resources are automatically released after completion  

### Best Practices for Batch Transform

- **Efficient Data Splitting**: Break data by time or ID for performance
- **Parallel Processing**: Use multiple instances for faster execution
- **Cleanup**: Remove unused intermediate S3 outputs
- **Cost Optimization**: Reuse model files across batch jobs to save cost

## Real-Time vs. Batch Inference: When to Use Each

| Scenario                        | Use Real-Time      | Use Batch Transform |
|--------------------------------|--------------------|---------------------|
| Low-latency needed             | ✅ Yes             | ❌ No               |
| Scheduled or delayed outcomes  | ❌ No              | ✅ Yes              |
| Heavy, infrequent workloads    | ❌ Not Ideal       | ✅ Yes              |
| Stable or variable load        | ✅ Auto-scalable    | ✅ Parallelizable   |
| High volume, cost-sensitive    | ❌ Costly          | ✅ Efficient         |

## Related AWS Services

- **Amazon SageMaker**  
  - Real-Time Endpoints: Constant, low-latency predictions  
  - Batch Transform: On-demand bulk processing  

- **Amazon CloudWatch**  
  - Monitor metrics: latency, CPU, memory, error rate  

- **AWS Lambda**  
  - Trigger logic before/after inference requests  

- **AWS Step Functions**  
  - Manage multi-step workflows or chained jobs  

- **SageMaker Model Monitor**  
  - Detect concept drift and anomalies in real-time or batch modes  
