

# 3.1 AWS Services for Model Deployment

- [3.1.1 Amazon Elastic Container Registry (Amazon ECR)](#311-amazon-elastic-container-registry-amazon-ecr)
- [3.1.2 Amazon S3 Event Notifications](#312-amazon-s3-event-notifications)
- [3.1.3 Amazon S3 Event Notifications with EventBridge](#313-amazon-s3-event-notifications-with-eventbridge)
- [3.1.4 AWS CodeArtifact](#314-aws-codeartifact)
- [3.1.5 AWS CodeBuild](#315-aws-codebuild)
- [3.1.6 AWS CodeDeploy](#316-aws-codedeploy)
- [3.1.7 AWS CodePipeline](#317-aws-codepipeline)
- [3.1.8 AWS Batch](#318-aws-batch)
- [3.1.9 AWS Step Functions for ML Orchestration](#319-aws-step-functions-for-ml-orchestration)
- [3.1.10 Amazon SageMaker](#3110-amazon-sagemaker)
- [3.1.11 AWS Lambda](#3111-aws-lambda)
- [3.1.12 ECS / EKS (Elastic Container & Kubernetes Services)](#3112-ecs--eks-elastic-container--kubernetes-services)
- [3.1.13 AWS Elastic Beanstalk](#3113-aws-elastic-beanstalk)
- [3.1.14 AWS App Runner](#3114-aws-app-runner)

---

## 3.1.1 Amazon Elastic Container Registry (Amazon ECR)

- Fully managed Docker image registry for storing, versioning, and managing container images.
- Used for custom ML models and automation jobs, often with SageMaker training/inference.
- Integrated with ECS (Elastic Container Service), backed by S3.
- Supports image versioning, access control, and lifecycle policies.
- Enables secure, scalable, and automated container workflows for ML deployment.

**Sample Scenario:**
- Use separate ECR repositories (Development, Staging, Production) to manage container images in deployment pipelines, with security layers for integrity.

---

## 3.1.2 Amazon S3 Event Notifications

- Allows S3 buckets to automatically trigger actions on events (object uploads, deletions, etc.).
- Sends notifications to Lambda, SQS, or SNS for reactive, event-driven workflows.
- Enables automation of downstream processing, data pipelines, and alerts.

**Common Event Types:**
- `s3:ObjectCreated:Put`
- `s3:ObjectCreated:Post`
- `s3:ObjectCreated:Copy`
- `s3:ObjectCreated:*`

---


## 3.1.3 Amazon S3 Event Notifications with EventBridge

- Extends S3 event notifications by integrating with Amazon EventBridge (serverless event bus).
- Enables advanced, serverless event-driven workflows and cross-service orchestration.
- EventBridge can trigger AWS Step Functions, Lambda, SNS, SQS, etc., for robust automation and error handling.
- Supports advanced filtering, cross-account, and multi-region event routing.

**Integration:** S3 + EventBridge → Lambda / Step Functions / SNS / SQS

**Steps:**
- Configure S3 to send events to EventBridge
- Set up EventBridge rules
- Choose targets (Lambda, SNS, etc.)
- Apply least-privilege permissions

---


## 3.1.4 AWS CodeArtifact

- Fully managed repository service for storing and managing software packages and dependencies.
- Supports PyPI, npm, Maven, NuGet, Conda formats.
- Enables reuse of standard libraries across ML models and environments.
- Tracks exact package versions for repeatable training and evaluation.
- Integrates with CodePipeline, CodeBuild, CodeDeploy for CI/CD.
- Fine-grained IAM permissions, encryption in transit and at rest.
- Share packages across accounts/regions; supports distributed ML teams.
- Local cache for frequently used packages; faster model training/deployment.

---


## 3.1.5 AWS CodeBuild

AWS CodeBuild is a fully managed CI service that compiles code, runs tests, and produces deployable artifacts. It is especially useful for automating ML workflows in CI/CD pipelines.

- Compiles Python ML scripts and other code; supports custom build environments (Docker).
- Runs unit/integration/data validation tests for high code quality.
- Triggers SageMaker training, evaluation, and deployment jobs.
- Installs consistent package versions from CodeArtifact or public repos for reproducibility.
- Creates model files, Docker images, and setup scripts; stores results in S3.
- Supports parallel/distributed builds for multiple datasets or hyperparameter combos.
- Integrates with CodePipeline for automated triggers (code commits, new data, etc.).
- Scales automatically to handle multiple builds simultaneously.

**Why CodeBuild for ML?**
- Automates repetitive build, test, and packaging steps for ML projects.
- Ensures consistency and reproducibility across environments.
- Reduces manual errors and accelerates the ML development lifecycle.

---


## 3.1.6 AWS CodeDeploy

- Deployment automation service for rolling out ML models to SageMaker Endpoints, EC2, Lambda, and edge devices.
- Ensures reliable, consistent, and fully automated model delivery, minimizing disruptions and rollback risks.
- Moves trained models to SageMaker Endpoints, batch transform jobs, or real-time inference services.
- Integrates with CodePipeline for CI/CD workflows.
- **Deployment Strategies:**
  - **Blue/Green:** Deploy new version (green) beside old (blue) with full rollback capability.
  - **Canary:** Gradually shift traffic to the new version to monitor for issues.
  - **Rolling Updates:** Deploy in small chunks to reduce service interruption.
- Works with SageMaker Model Registry for version tracking and approval-based deployment.
- Monitors via CloudWatch; auto-rollback on errors or degraded metrics.

---


## 3.1.7 AWS CodePipeline

- Managed CI/CD service automating the full lifecycle of ML models (data prep to deployment).
- Orchestrates complex ML workflows with zero manual intervention.
- Handles data cleaning, feature engineering, model training, testing, packaging, and deployment.
- **Multi-Stage Pipeline Support:**
  1. **Source:** Retrieves updated source code or ML model artifacts (GitHub, CodeCommit, S3, etc.).
  2. **Build:** Uses CodeBuild to install dependencies, compile code, run tests.
  3. **Training:** Triggers SageMaker Pipelines or Processing Jobs for retraining.
  4. **Deployment:** Deploys models using SageMaker Endpoints via CodeDeploy.
- **Event-Driven Execution:** Triggers on S3 data arrival, Git pushes, or model registry updates.
- Integrates with SageMaker Pipelines for end-to-end ML process management.
- Supports blue/green, canary, and rolling deployments for smooth transitions.
- Stores models, logs, and metadata in S3 for reproducibility.
- Monitors with CloudWatch, sends alerts via SNS.

---


## 3.1.8 AWS Batch

- **Managed batch computing service** for running containerized ML batch inference jobs.
- Designed for **flexible resource allocation** and **automatic scaling**.
- Integrates natively with **ECS** for job scheduling, queueing, and execution.
- Seamless integration with **S3** for input/output data storage in batch workflows.
- More flexible than **ECS/EKS** for batch; more customizable than **SageMaker batch transform**.
- Handles **dependent jobs** (e.g., data preprocessing → model training) for optimized performance and cost.
- **EC2 Spot Instances** offer up to **90% cost savings** for interruption-tolerant, high-volume ML training.

---


## 3.1.9 AWS Step Functions for ML Orchestration

- **Fully managed serverless orchestration service** for automating ML pipelines (preprocessing, training, evaluation, deployment, monitoring).
- Supports **sequential, parallel, and branching workflows**; **built-in error handling and retry**.
- Use **parallel states** for concurrent model training; **task states** for data processing.
- **State machines** invoke SageMaker jobs, Lambda, etc.
- Integrates with **SageMaker, Lambda, S3, SNS/SQS, Glue**.
- **Visual workflow design, execution tracking, debugging**.
- Supports **human-in-the-loop** (SNS, SQS, Lambda for manual steps).
- Coordinates across AWS services for **full ML lifecycle automation**.
- Use **managed spot training in SageMaker** for scalable, cost-optimized pipelines.

**Summary Table:**

| **Feature**                    | **Description**                                 |
|------------------------------|--------------------------------------------------|
| **End-to-End ML Workflow**    | Automates entire ML process                      |
| **Built-in Retry & Error Catch** | Resilient and reliable workflow execution     |
| **Visual Interface**          | Easy workflow design and debugging               |
| **Human Approval Support**    | Manual steps before sensitive actions            |
| **Multi-Service Integration** | Works across multiple AWS services seamlessly    |


**When to Use Step Functions for ML?**
- **Automate multi-step ML workflows**
- **Add monitoring, retry, and rollback logic**
- **Combine SageMaker, Lambda, S3, etc.**
- **Implement CI/CD for ML models**
- **Provide manual approvals before deployment**

---




## 3.1.10 Amazon SageMaker

**Amazon SageMaker** is AWS’s all-in-one machine learning platform for:

- **Data labeling**
- **Model training** (using built-in algorithms or your own)
- **Deployment** through managed endpoints

With **SageMaker Model Hosting**, you can deploy models as REST endpoints with:

- **Hardware provisioning**
- **Auto-scaling**
- **Load management**

### Why Use SageMaker for Deployment?

- **Innate Scaling:** Automatically adjusts capacity based on request volume.
- **Reduced Ops Overhead:** Handles infrastructure, networking, and updates.
- **Tracking & Monitoring:** Integrates with **Amazon CloudWatch** for metrics like latency and error rates.
- **Model Versioning & Rollbacks:** Use the **SageMaker Model Registry** to manage and revert versions.

### SageMaker Deployment Options

- **Real-Time Endpoints:** Fast, low-latency, continuous predictions.
- **Batch Transform:** Scheduled or bulk predictions (e.g., hourly jobs).
- **Asynchronous Inference:** For long-running inference tasks with deferred response.
- **Serverless Inference:** Zero-capacity during idle time; scales automatically when requests increase.

---

## 3.1.11 AWS Lambda

**AWS Lambda** is a serverless compute service ideal for lightweight, event-driven ML inference tasks.

**Best for:**
- Lightweight, event-driven tasks
- Pre-/post-processing for ML pipelines

**Advantages:**
- Fully serverless (no infrastructure management)
- Pay-per-call pricing model
- Automatic scaling for unpredictable workloads

**Limitations:**
- Not suitable for large models or high-throughput inference
- Cold start latency for infrequently used functions

**Typical Usage Pattern:**
1. Package your ML model and inference code as a Lambda function
2. Trigger via API Gateway, S3 events, or other AWS services
3. Lambda loads the model (from S3, EFS, or container image), runs inference, and returns the result

**Best Practice:**
- Use Lambda for small, stateless models or preprocessing/postprocessing steps
- For larger models or real-time, high-throughput inference, use SageMaker Endpoints or ECS/EKS

**Scenario Example:**
> Use Lambda to trigger lightweight fraud detection on new transactions uploaded to S3. For heavy, continuous inference, switch to SageMaker or ECS.

### Model Serving Workflow

For infrequent inference tasks, Lambda is more cost-efficient than running SageMaker 24/7.

**Architecture:**
- Lambda function holds the model or pulls it from **S3**, **EFS**, or **ECR**
- Triggers: **S3 events**, **API Gateway**, **EventBridge**
- Performs inference, then terminates

**Limitations**

- Unsuitable for large models or heavy workloads
- Not ideal for batch or real-time high-volume inference

**Trade-Offs**

| Limitation      | Notes                                              |
|-----------------|----------------------------------------------------|
| Cold Starts     | Delay during first load, especially for large models |
| Memory Limits   | Max 10GB RAM – may be too low for some models      |
| Stateless       | Unsuitable for stateful or GPU-heavy workloads     |

---
### 3.1.11 ECS / EKS (Elastic Container & Kubernetes Services)

- **Best for**: Custom container setups or microservices architectures
- **Pros**:
  - Full control over infrastructure
  - Works well for advanced ML environments
- **Cons**:
  - Higher complexity and manual management

**Kubernetes Autoscaling Components:**
- **Vertical Pod Autoscaler (VPA):** Automatically adjusts CPU/memory requests and limits for pods.
- **Horizontal Pod Autoscaler (HPA):** Scales the number of pod replicas based on metrics (CPU, custom, etc.).
- **Cluster Autoscaler:** Adjusts the number of nodes in a cluster based on pod scheduling needs.

Using Cluster Autoscaler with HPA, and basing HPA on custom metrics (e.g., GPU usage, memory, business metrics), leads to more efficient scaling both horizontally (pods) and vertically (nodes).

---
### 3.1.10.3 AWS Elastic Beanstalk

- **Best for**: Web apps that include ML models
- **Pros**:
  - Easy setup and deployment
  - Automatically provisions EC2, load balancers, etc.
- **Cons**:
  - Not tailored for ML workflows

---

### 3.1.10.4 AWS App Runner

- **Best for**: Simple web-based predictions using containers
- **Pros**:
  - Fully managed, auto-scaled container service
- **Cons**:
  - Lacks ML-specific features like monitoring and model registry
