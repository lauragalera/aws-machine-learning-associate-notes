# Model and Endpoint Requirements

In SageMaker, **models** define the algorithmic logic, while **endpoints** act as hosted interfaces that applications (web, mobile, IoT) use to request inferences. A model must be deployed to an endpoint before it can serve predictions.

## SageMaker Endpoint Types

### 1. Real-Time Endpoints

- **Persistent (Provisioned) Endpoints** designed for fast, continuous predictions.
- Applications with low-latency needs (e.g., e-commerce, fraud detection).

**When to Use:**
- High-volume applications needing **milliseconds-level** response.
- Systems with strict SLAs for latency.

**Benefits:**
- **Steady Performance** – Predictable latency
- **Auto Scaling** – Adjusts instance count based on real-time traffic
- **Automatic Resource Adjustment** – Scales horizontally to match demand

**Limitations:**
- **Continuous Costs** – Charged even when idle
- **Manual Scaling Config** – Scaling strategies need tuning to avoid resource waste or slowdowns

### 2. Serverless Endpoints

- Ideal for unpredictable, sporadic traffic
- **No need to manage infrastructure**; compute power auto-adjusts
- AWS handles provisioning based on request volume

**When to Use:**
- Applications with **intermittent or bursty traffic**
- **Small to medium-sized** models with infrequent use

**Benefits:**
- **No Instance Management**
- **Pay per Inference** – No charges during idle time

**Limitations:**
- **Cold Start Latency** – First requests may take longer
- **Memory Limits** – Only supports 1–6 GB memory (in 1 GB increments)

### 3. Asynchronous Inference

Amazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.

Asynchronous inference is suited for long-running tasks, such as processing large files, and scales resources based on demand. It also allows auto-scaling to zero when there are no requests, making it cost-effective.

- Used for **long-running inference jobs**
- Processes inputs **asynchronously** to avoid blocking client apps

**When to Use:**
- Long processing tasks like:
  - Batch inferences
  - High-quality image/video generation
  - Full content generation

**Benefits:**
- **No Blocking** – Requests are queued and processed in the background
- **Callback & Notifications** – Output can be sent when ready

**Limitations:**
- **Higher Latency** – Queueing introduces delays
- **Requires S3** – Input/output must be handled through Amazon S3

## Importance of Understanding Endpoint Requirements

### Key Considerations:

- **Scalability**
  - Endpoints must scale up during peak load and scale down when idle.
  - Configure **auto-scaling** for optimal performance.

- **Performance**
  - Choose correct **instance types**
  - Match latency and throughput goals

- **Cost Efficiency**
  - Avoid over-provisioning
  - Align infrastructure with traffic patterns

- **Dependability**
  - Ensure **high availability**
  - Design for fault tolerance and resilience

## Summary Table: Endpoint Types

| Type                | Use Case                                | Pros                               | Cons                                |
|---------------------|------------------------------------------|------------------------------------|-------------------------------------|
| Real-Time           | Immediate predictions; high traffic      | Low latency, auto-scaling          | Always-on cost, scaling config      |
| Serverless          | Unpredictable or low-volume traffic      | No instance mgmt, pay-per-use      | Cold starts, memory limitations     |
| Asynchronous        | Long-running inference tasks             | Non-blocking, notification support | High latency, requires S3 storage   |

## Deploying Endpoints Inside a VPC

### Benefits:
- Blocks public internet access.
- Fine-grained access control via **Security Groups** and **Network ACLs**.
- Enables access to private VPC resources (e.g., internal apps, databases).

### Steps:
1. **Create a VPC** with private subnets and strict security groups.
2. **Specify `VpcConfig`** during endpoint deployment.
3. **Attach IAM Role** with VPC permissions.
4. **Verify connectivity** using VPC flow logs and internal tests.

## Best Practices

- **Multi-Model Endpoints**: Deploy multiple models on one endpoint to reduce costs.
- **Select Proper Instance Types**:
  - Start small: `ml.m5.large`
  - For deep learning: `ml.p3`, `ml.g4dn`
- **Enable SageMaker Model Monitor** to detect:
  - Data skew
  - Concept drift

  ## Elastic Inference

  SageMaker Endpoint Elastic Inference attaches fractional GPU-powered acceleration to your endpoint, significantly reducing computation time per prediction and thus lowering endpoint latency without the cost of full GPU instances.

  ## 3.6.3 SageMaker Endpoints: Hosting & Features

SageMaker endpoints allow models to be deployed as a **REST API**.


### 3.6.3.1 Endpoint Configuration
- Model path
- Instance type
- Initial scaling settings
- Optional encryption

### 3.6.3.2 Key Features
- **Autoscaling**: Based on requests or latency thresholds
- **Multi-Model Endpoints**: Host multiple models on one endpoint
- **Traffic Shifting**: Smooth rollouts by routing partial traffic
- **Secure Access**: IAM, VPC, and SSL integration

### 3.6.3.3 SageMaker Autoscaling
Amazon SageMaker supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.

Enabling Auto Scaling allows the endpoint to dynamically adjust the number of instances based on actual traffic. By targeting instance utilization, the deployment can automatically scale out during peak times and scale in during low demand, improving both response time and throughput without over-provisioning. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.

### 3.6.3.4 Multi-Model Endpoints

Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models.

Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances.

Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints.

Multi-model endpoints are useful for hosting multiple models on a single instance to optimize resource usage, but they do not address scaling or performance issues for a single model under traffic spikes.