# 3.6 Model and Endpoint Requirements

- [3.6 Model and Endpoint Requirements](#36-model-and-endpoint-requirements)
- [3.6.1 SageMaker Endpoint Types](#361-sagemaker-endpoint-types)
  - [Real-Time Endpoints](#real-time-endpoints)
  - [Serverless Endpoints](#serverless-endpoints)
  - [Asynchronous Inference](#asynchronous-inference)
- [3.6.2 Importance of Understanding Endpoint Requirements](#362-importance-of-understanding-endpoint-requirements)
  - [Key Considerations](#key-considerations)
  - [Summary Table: Endpoint Types](#summary-table-endpoint-types)
- [3.6.3 Deploying Endpoints Inside a VPC](#363-deploying-endpoints-inside-a-vpc)
  - [Benefits](#benefits)
  - [Steps](#steps)
- [Best Practices](#best-practices)
- [3.6.4 SageMaker Endpoints: Hosting & Features](#364-sagemaker-endpoints-hosting--features)
  - [Endpoint Configuration](#endpoint-configuration)
  - [Key Features](#key-features)
  - [Multi-Model Endpoints](#multi-model-endpoints)

---

In SageMaker, **models** define the algorithmic logic, while **endpoints** act as hosted interfaces for applications (web, mobile, IoT) to request inferences. A model must be deployed to an endpoint before it can serve predictions.

## 3.6.1 SageMaker Endpoint Types

### Real-Time Endpoints

- **Persistent (Provisioned) Endpoints** for fast, continuous predictions
- Used by applications with low-latency needs (e.g., e-commerce, fraud detection)

**When to Use:**
- High-volume applications needing **milliseconds-level** response
- Systems with strict SLAs for latency

**Benefits:**
- **Steady Performance** – Predictable latency
- **Auto Scaling** – Adjusts instance count based on real-time traffic
- **Automatic Resource Adjustment** – Scales horizontally to match demand

**Limitations:**
- **Continuous Costs** – Charged even when idle
- **Manual Scaling Config** – Scaling strategies need tuning to avoid resource waste or slowdowns

### Serverless Endpoints

- Ideal for unpredictable, sporadic traffic
- **No need to manage infrastructure**; compute power auto-adjusts
- AWS handles provisioning based on request volume

**When to Use:**
- Applications with **intermittent or bursty traffic**
- **Small to medium-sized** models with infrequent use

**Benefits:**
- **No Instance Management**
- **Pay per Inference** – No charges during idle time

**Limitations:**
- **Cold Start Latency** – First requests may take longer
- **Memory Limits** – Only supports 1–6 GB memory (in 1 GB increments)

### Asynchronous Inference

Amazon SageMaker Asynchronous Inference is designed for workloads with large payloads (up to 1GB), long processing times (up to one hour), and near real-time latency needs. Requests are queued and processed in the background, allowing endpoints to autoscale to zero when idle—saving costs.

**Key Features:**
- Handles long-running inference jobs without blocking client applications
- Supports callback and notification when output is ready
- Input/output managed via Amazon S3

**When to Use:**
- Batch inferences
- High-quality image/video generation
- Full content generation

**Benefits:**
- **Cost-Efficient** – Autoscaling to zero when idle
- **Non-Blocking** – Client apps are not held up by long jobs
- **Notification Support** – Output can be sent when ready

**Limitations:**
- **Higher Latency** – Queueing and processing may introduce delays
- **Requires S3** – All input/output must be stored in Amazon S3
---
## 3.6.2 Importance of Understanding Endpoint Requirements

### Key Considerations:

- **Scalability**
  - Endpoints must scale up during peak load and scale down when idle.
  - Configure **auto-scaling** for optimal performance.

- **Performance**
  - Choose correct **instance types**
  - Match latency and throughput goals

- **Cost Efficiency**
  - Avoid over-provisioning
  - Align infrastructure with traffic patterns

- **Dependability**
  - Ensure **high availability**
  - Design for fault tolerance and resilience

### Summary Table: Endpoint Types

| Type                | Use Case                                | Pros                               | Cons                                |
|---------------------|------------------------------------------|------------------------------------|-------------------------------------|
| Real-Time           | Immediate predictions; high traffic      | Low latency, auto-scaling          | Always-on cost, scaling config      |
| Serverless          | Unpredictable or low-volume traffic      | No instance mgmt, pay-per-use      | Cold starts, memory limitations     |
| Asynchronous        | Long-running inference tasks             | Non-blocking, notification support | High latency, requires S3 storage   |
---
## 3.6.3 Deploying Endpoints Inside a VPC

### Benefits:
- Blocks public internet access.
- Fine-grained access control via **Security Groups** and **Network ACLs**.
- Enables access to private VPC resources (e.g., internal apps, databases).

### Steps:
1. **Create a VPC** with private subnets and strict security groups.
2. **Specify `VpcConfig`** during endpoint deployment.
3. **Attach IAM Role** with VPC permissions.
4. **Verify connectivity** using VPC flow logs and internal tests.

## Best Practices

- **Multi-Model Endpoints**: Deploy multiple models on one endpoint to reduce costs.
- **Select Proper Instance Types**:
  - Start small: `ml.m5.large`
  - For deep learning: `ml.p3`, `ml.g4dn`
- **Enable SageMaker Model Monitor** to detect:
  - Data skew
  - Concept drift
---
## 3.6.4 SageMaker Endpoints: Hosting & Features

SageMaker endpoints allow models to be deployed as a **REST API**.

### Endpoint Configuration
- Model path
- Instance type
- Initial scaling settings
- Optional encryption

### Key Features
- **Autoscaling**: Based on requests or latency thresholds
- **Multi-Model Endpoints**: Host multiple models on one endpoint
- **Traffic Shifting**: Smooth rollouts by routing partial traffic
- **Secure Access**: IAM, VPC, and SSL integration
- **Elastic Inference**: Attaches fractional GPU-powered acceleration to your endpoint

#### Multi-Model Endpoints

Multi-Model Endpoints allow you to host multiple models on a single endpoint, sharing resources and reducing costs.

**Key Features:**
- Host many models using the same ML framework in a shared serving container
- Efficiently serve a mix of frequently and infrequently accessed models
- Support both CPU and GPU-backed models
- Enable time-sharing of memory resources across models

**Best Use Cases:**
- Large number of models with similar size and latency requirements
- Applications tolerant of occasional cold start latency (when infrequently used models are loaded)

**Limitations:**
- Not ideal for models with very high TPS or strict latency requirements—use dedicated endpoints instead
- Does not solve scaling or performance issues for a single model under traffic spikes