# Provisioning Compute Resources

This chapter covers strategies for choosing between **CPU** and **GPU** instances for training and inference in AWS SageMaker. The focus is on cost-efficiency, performance needs, and environment management.

## Choosing Between CPU and GPU

### CPU Instances

**Characteristics:**
- **General Purpose** – Ideal for basic ML tasks, data cleaning, and small-scale inference
- **Lower Cost** – Suitable for dev/testing, prototypes, and experimentation
- **Wide Availability** – T3, M5, C5 families cover a range of compute and memory options

**When to Use CPUs:**
- Light ML models (e.g., linear regression, decision trees)
- Development and early prototyping
- Low-throughput inference (occasional/batch requests)

### GPU Instances

**Characteristics:**
- **Parallel Processing** – Optimized for vector/matrix operations; essential for deep learning
- **High Throughput** – Speeds up training of large models like CNNs or Transformers
- **Specialized Instances** – Use P-series (e.g., `p3`, `p4`) or G-series (e.g., `g4`, `g5`) for intensive workloads

**When to Use GPUs:**
- Deep learning (vision, NLP, recommendation systems)
- High-volume, real-time inference (e.g., BERT/GPT models)
- Projects with strict training deadlines

## Decision Factors: CPU vs. GPU

| Factor                    | Choose CPU                             | Choose GPU                                   |
|--------------------------|----------------------------------------|----------------------------------------------|
| Model Complexity         | Simple or classical ML models          | Large deep learning models                   |
| Cost Sensitivity         | Lower cost, dev/test use               | Higher cost, performance-critical workloads  |
| Inference Volume         | Low-throughput, batch processing       | High-volume, low-latency requirements        |
| Time Constraints         | Non-urgent workloads                   | Time-sensitive training or inference         |

## Managing Compute Resources Across Environments

### Development & Testing
- Use smaller CPU instances: `t3.medium`, `m5.large`
- Consider **spot instances** for cost savings

### Staging
- Mimic production setup with smaller instances
- Use to test deployment logic and evaluate performance

### Production
- Scale **up or out** based on performance and cost needs
- Choose **robust CPU or GPU clusters** as required

## Elastic Scalability in SageMaker

### Real-Time Endpoints
- Use **SageMaker Automatic Scaling**
- Scale based on:
  - CPU/GPU utilization
  - Invocation counts
  - Latency metrics

### Batch Workloads
- Spin up GPU clusters only during job runtime
- Use **Amazon EC2 Spot Instances** to reduce costs for:
  - Non-critical tasks
  - Retriable workloads (e.g., dev training jobs, internal analytics)

## Monitoring and Optimization

### Using Amazon CloudWatch
- Track:
  - CPU/GPU utilization
  - Memory usage
  - Latency and I/O metrics
- Set up **alerts** to trigger autoscaling or configuration changes

### Experimentation
- Use **Amazon SageMaker Experiments** or manual tracking
- Monitor:
  - Instance types
  - Hyperparameters
  - Training duration and performance metrics
- Goal: Find the best **performance vs. cost** trade-off