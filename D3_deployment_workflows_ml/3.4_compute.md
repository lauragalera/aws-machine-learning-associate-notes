# Provisioning Compute Resources

This chapter covers strategies for choosing between **CPU** and **GPU** instances for training and inference in AWS SageMaker. The focus is on cost-efficiency, performance needs, and environment management.

## Choosing Between CPU and GPU

### CPU Instances

**Characteristics:**
- **General Purpose** – Ideal for basic ML tasks, data cleaning, and small-scale inference
- **Lower Cost** – Suitable for dev/testing, prototypes, and experimentation
- **Wide Availability** – T3, M5, C5 families cover a range of compute and memory options

**When to Use CPUs:**
- Light ML models (e.g., linear regression, decision trees)
- Development and early prototyping
- Low-throughput inference (occasional/batch requests)

### GPU Instances

**Characteristics:**
- **Parallel Processing** – Optimized for vector/matrix operations; essential for deep learning
- **High Throughput** – Speeds up training of large models like CNNs or Transformers
- **Specialized Instances** – Use P-series (e.g., `p3`, `p4`) or G-series (e.g., `g4`, `g5`) for intensive workloads

**When to Use GPUs:**
- Deep learning (vision, NLP, recommendation systems)
- High-volume, real-time inference (e.g., BERT/GPT models)
- Projects with strict training deadlines

## Decision Factors: CPU vs. GPU

| Factor                    | Choose CPU                             | Choose GPU                                   |
|--------------------------|----------------------------------------|----------------------------------------------|
| Model Complexity         | Simple or classical ML models          | Large deep learning models                   |
| Cost Sensitivity         | Lower cost, dev/test use               | Higher cost, performance-critical workloads  |
| Inference Volume         | Low-throughput, batch processing       | High-volume, low-latency requirements        |
| Time Constraints         | Non-urgent workloads                   | Time-sensitive training or inference         |

## Managing Compute Resources Across Environments

### Development & Testing
- Use smaller CPU instances: `t3.medium`, `m5.large`
- Consider **spot instances** for cost savings

### Staging
- Mimic production setup with smaller instances
- Use to test deployment logic and evaluate performance

### Production
- Scale **up or out** based on performance and cost needs
- Choose **robust CPU or GPU clusters** as required

## Elastic Scalability in SageMaker

### Real-Time Endpoints
- Use **SageMaker Automatic Scaling**
- Scale based on:
  - CPU/GPU utilization
  - Invocation counts
  - Latency metrics

Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.

### Batch Workloads
- Spin up GPU clusters only during job runtime
- Use **Amazon EC2 Spot Instances** to reduce costs for:
  - Non-critical tasks
  - Retriable workloads (e.g., dev training jobs, internal analytics)

## Monitoring and Optimization

### Using Amazon CloudWatch
- Track:
  - CPU/GPU utilization
  - Memory usage
  - Latency and I/O metrics
- Set up **alerts** to trigger autoscaling or configuration changes

### Experimentation
- Use **Amazon SageMaker Experiments** or manual tracking
- Monitor:
  - Instance types
  - Hyperparameters
  - Training duration and performance metrics
- Goal: Find the best **performance vs. cost** trade-off

# Instance Types and Their Impact on Performance

## Memory-Optimized Instances: For Models with Large Memory Requirements

**Use When:** Models handle large datasets, perform complex operations, or require significant in-memory storage.

### Key Features
- High memory-to-CPU ratio.
- Supports deep learning, analytics, and in-memory processing.
- Ideal for large batch sizes or models with extensive features.

### Use Cases
- NLP models (e.g., BERT, GPT).
- Large-scale data preprocessing.

### Example Instance Types
- `R5`: Ideal for large datasets and analytics.
- `X1e`: Best for memory-bound workloads like HPC or in-memory databases.

## Compute-Optimized Instances: For CPU-Intensive Workloads

**Use When:** Workloads require significant processing power but moderate memory.

### Key Features
- High CPU-to-memory ratio.
- Efficient for parallel computation and simulations.

### Use Cases
- Training simpler models (e.g., decision trees, linear regression).
- Real-time analytics and scientific computing.

### Example Instance Types
- `C5` (e.g., `c5.xlarge`, `c5.2xlarge`): Great for heavy CPU-bound ML tasks.
- `C6g`: ARM-based, cost-efficient, high-performance CPU computing.

## Inference-Optimized Instances: For Faster Prediction and Model Inference

**Use When:** Serving ML models in production, especially for real-time predictions.

### Key Features
- Optimized for low latency and high throughput.
- Scales well with high request volumes.

### Use Cases
- Real-time recommender systems.
- Computer vision (e.g., image classification, object detection).
- Chatbots and virtual assistants.

### Example Instance Types
- `Inf1`: Powered by AWS Inferentia, low-cost, high-performance inference.
- `G4ad`: GPU-based, ideal for real-time multimedia and ML inference.

