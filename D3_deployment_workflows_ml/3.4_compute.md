
# 3.4 Compute Resources for AWS ML Deployment

- [3.4.1 Choosing Between CPU and GPU](#341-choosing-between-cpu-and-gpu)
  - [CPU Instances](#cpu-instances)
  - [GPU Instances](#gpu-instances)
  - [Decision Factors: CPU vs. GPU](#decision-factors-cpu-vs-gpu)
- [3.4.2 Managing Compute Resources Across Environments](#342-managing-compute-resources-across-environments)
- [3.4.3 Elastic Scalability in SageMaker](#343-elastic-scalability-in-sagemaker)
- [3.4.4 Monitoring and Optimization](#344-monitoring-and-optimization)
- [3.4.5 Instance Types and Their Impact](#345-instance-types-and-their-impact)
  - [Memory-Optimized Instances](#memory-optimized-instances)
  - [Compute-Optimized Instances](#compute-optimized-instances)
  - [Inference-Optimized Instances](#inference-optimized-instances)

---

Selecting the right compute resources is critical for balancing **cost, performance, and scalability** in AWS ML workflows. This section summarizes best practices for choosing and managing compute in SageMaker.

## 3.4.1 Choosing Between CPU and GPU

### CPU Instances

**Characteristics:**
- **General Purpose:** Basic ML tasks, data cleaning, small-scale inference
- **Lower Cost:** Dev/testing, prototypes, experimentation
- **Wide Availability:** T3, M5, C5 families

**When to Use CPUs:**
- Light ML models (e.g., linear regression, decision trees)
- Development and prototyping
- Low-throughput or batch inference

### GPU Instances

**Characteristics:**
- **Parallel Processing:** Vector/matrix ops, deep learning
- **High Throughput:** Fast training for large models (CNNs, Transformers)
- **Specialized:** P-series (`p3`, `p4`), G-series (`g4`, `g5`)

**Best Practice:**
- Use Sagemaker `p4d` for training (NVIDIA A100 GPUs)
- Use `ml.inf1` for inference (AWS Inferentia chips)
- Match compute to workflow phase: GPU for training, Inferentia for real-time inference

**When to Use GPUs:**
- Deep learning (vision, NLP, recommendation)
- High-volume, real-time inference (e.g., BERT/GPT)
- Strict training deadlines

### Decision Factors: CPU vs. GPU

| Factor            | Choose CPU                        | Choose GPU                                 |
|-------------------|-----------------------------------|--------------------------------------------|
| Model Complexity  | Simple/classical ML               | Large deep learning                        |
| Cost Sensitivity  | Lower cost, dev/test              | Higher cost, perf.-critical                |
| Inference Volume  | Low-throughput, batch             | High-volume, low-latency                   |
| Time Constraints  | Non-urgent                        | Time-sensitive training/inference          |

---
## 3.4.2 Managing Compute Resources Across Environments

**Development & Testing:**
- Use small CPU instances (`t3.medium`, `m5.large`)
- Consider **spot instances** for cost savings

**Staging:**
- Mimic production with smaller instances
- Test deployment logic and performance

**Production:**
- Scale up/out for performance and cost
- Use robust CPU or GPU clusters as needed

---
## 3.4.3 Elastic Scalability in SageMaker

### Real-Time Endpoints
- Use **SageMaker Automatic Scaling**
- Scale on CPU/GPU utilization, invocation count, latency
- Fully managed endpoints for low-latency, real-time inference

### Batch Workloads
- Spin up GPU clusters only during job runtime
- Use **EC2 Spot Instances** for non-critical or retriable jobs

---
## 3.4.4 Monitoring and Optimization

### Using Amazon CloudWatch
- Track CPU/GPU utilization, memory, latency, I/O
- Set up **alerts** for autoscaling or config changes
- Real-time monitoring of endpoint metrics (invocation count, latency)

### Experimentation
- Use **SageMaker Experiments** or manual tracking
- Monitor instance types, hyperparameters, training duration, performance
- Goal: Find best **performance vs. cost**
- **Trackers**: Log metadata for each trial (hyperparameters, datasets, code changes)
- **Experiments**: Group multiple trials for easy comparison and reproducibility

---
## 3.4.5 Instance Types and Their Impact

### Memory-Optimized Instances

**Use When:** Large datasets, complex ops, high in-memory needs

**Key Features:**
- High memory-to-CPU ratio
- Supports deep learning, analytics, in-memory processing
- Ideal for large batch sizes, extensive features

**Use Cases:**
- NLP models (BERT, GPT)
- Large-scale data preprocessing

**Examples:**
- `R5`: Large datasets, analytics
- `X1e`: Memory-bound workloads, HPC, in-memory DBs

### Compute-Optimized Instances

**Use When:** High processing, moderate memory

**Key Features:**
- High CPU-to-memory ratio
- Efficient for parallel computation, simulations

**Use Cases:**
- Training simple models (decision trees, linear regression)
- Real-time analytics, scientific computing

**Examples:**
- `C5` (e.g., `c5.xlarge`, `c5.2xlarge`): CPU-bound ML
- `C6g`: ARM-based, cost-efficient, high-perf CPU

###  Inference-Optimized Instances

**Use When:** Production inference, real-time predictions

**Key Features:**
- Low latency, high throughput
- Scales for high request volume

**Use Cases:**
- Real-time recommender systems
- Computer vision (image classification, object detection)
- Chatbots, virtual assistants

**Examples:**
- `Inf1`: AWS Inferentia, low-cost, high-perf inference
- `G4ad`: GPU-based, real-time multimedia/ML inference

