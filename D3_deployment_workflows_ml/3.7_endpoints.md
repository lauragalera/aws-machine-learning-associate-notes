# Model and Endpoint Requirements

In SageMaker, **models** define the algorithmic logic, while **endpoints** act as hosted interfaces that applications (web, mobile, IoT) use to request inferences. A model must be deployed to an endpoint before it can serve predictions.

## SageMaker Endpoint Types

### 1. Real-Time Endpoints

- **Persistent (Provisioned) Endpoints** designed for fast, continuous predictions.
- Applications with low-latency needs (e.g., e-commerce, fraud detection).

**When to Use:**
- High-volume applications needing **milliseconds-level** response.
- Systems with strict SLAs for latency.

**Benefits:**
- **Steady Performance** – Predictable latency
- **Auto Scaling** – Adjusts instance count based on real-time traffic
- **Automatic Resource Adjustment** – Scales horizontally to match demand

**Limitations:**
- **Continuous Costs** – Charged even when idle
- **Manual Scaling Config** – Scaling strategies need tuning to avoid resource waste or slowdowns

### 2. Serverless Endpoints

- Ideal for unpredictable, sporadic traffic
- **No need to manage infrastructure**; compute power auto-adjusts
- AWS handles provisioning based on request volume

**When to Use:**
- Applications with **intermittent or bursty traffic**
- **Small to medium-sized** models with infrequent use

**Benefits:**
- **No Instance Management**
- **Pay per Inference** – No charges during idle time

**Limitations:**
- **Cold Start Latency** – First requests may take longer
- **Memory Limits** – Only supports 1–6 GB memory (in 1 GB increments)

### 3. Asynchronous Inference

Amazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.

Asynchronous inference is suited for long-running tasks, such as processing large files, and scales resources based on demand. It also allows auto-scaling to zero when there are no requests, making it cost-effective.

- Used for **long-running inference jobs**
- Processes inputs **asynchronously** to avoid blocking client apps

**When to Use:**
- Long processing tasks like:
  - Batch inferences
  - High-quality image/video generation
  - Full content generation

**Benefits:**
- **No Blocking** – Requests are queued and processed in the background
- **Callback & Notifications** – Output can be sent when ready

**Limitations:**
- **Higher Latency** – Queueing introduces delays
- **Requires S3** – Input/output must be handled through Amazon S3

## Importance of Understanding Endpoint Requirements

### Key Considerations:

- **Scalability**
  - Endpoints must scale up during peak load and scale down when idle.
  - Configure **auto-scaling** for optimal performance.

- **Performance**
  - Choose correct **instance types**
  - Match latency and throughput goals

- **Cost Efficiency**
  - Avoid over-provisioning
  - Align infrastructure with traffic patterns

- **Dependability**
  - Ensure **high availability**
  - Design for fault tolerance and resilience

## Summary Table: Endpoint Types

| Type                | Use Case                                | Pros                               | Cons                                |
|---------------------|------------------------------------------|------------------------------------|-------------------------------------|
| Real-Time           | Immediate predictions; high traffic      | Low latency, auto-scaling          | Always-on cost, scaling config      |
| Serverless          | Unpredictable or low-volume traffic      | No instance mgmt, pay-per-use      | Cold starts, memory limitations     |
| Asynchronous        | Long-running inference tasks             | Non-blocking, notification support | High latency, requires S3 storage   |

## Deploying Endpoints Inside a VPC

### Benefits:
- Blocks public internet access.
- Fine-grained access control via **Security Groups** and **Network ACLs**.
- Enables access to private VPC resources (e.g., internal apps, databases).

### Steps:
1. **Create a VPC** with private subnets and strict security groups.
2. **Specify `VpcConfig`** during endpoint deployment.
3. **Attach IAM Role** with VPC permissions.
4. **Verify connectivity** using VPC flow logs and internal tests.

## Best Practices

- **Multi-Model Endpoints**: Deploy multiple models on one endpoint to reduce costs.
- **Select Proper Instance Types**:
  - Start small: `ml.m5.large`
  - For deep learning: `ml.p3`, `ml.g4dn`
- **Enable SageMaker Model Monitor** to detect:
  - Data skew
  - Concept drift

  ## Elastic Inference

  SageMaker Endpoint Elastic Inference attaches fractional GPU-powered acceleration to your endpoint, significantly reducing computation time per prediction and thus lowering endpoint latency without the cost of full GPU instances.