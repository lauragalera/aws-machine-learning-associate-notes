# 3.7 Containerization Concepts


- [3.7.1 What is Containerization?](#371-what-is-containerization)
- [3.7.2 AWS Container Services](#372-aws-container-services)
  - [ECS](#ecs)
  - [EKS](#eks)
  - [ECR](#ecr)
- [3.7.3 Sample Dockerfile for ML Inference](#373-sample-dockerfile-for-ml-inference)
- [3.7.4 Container-Based Deployment Patterns](#374-container-based-deployment-patterns)
- [3.7.5 Choosing Appropriate Containers](#375-choosing-appropriate-containers)
  - [Pre-built Containers](#1-pre-built-containers)
  - [Customized Containers](#2-customized-containers)
- [3.7.6 Containerization for Edge Devices with SageMaker Neo](#376-containerization-for-edge-devices-with-sagemaker-neo)
- [3.7.7 Multi-Model vs Multi-Container Deployment](#377-multi-model-vs-multi-container-deployment)

---

## 3.7.1 What is Containerization?

Containerization packages an ML model and its dependencies (e.g., Python libraries, frameworks like TensorFlow or PyTorch, and custom scripts) into a **self-contained, portable unit**. This ensures it can run **consistently** across different environments (local dev, ECS, EKS, etc.).


| Benefit       | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| **Portability** | Same image can run on local machine, ECS, or EKS with minimal changes     |
| **Isolation**   | Each container runs independently with its own dependencies               |
| **Scalability** | ECS/EKS can auto-scale based on traffic, CPU, memory                      |
| **Standardization** | Docker image format simplifies sharing across teams and environments   |
---
## 3.7.2 AWS Container Services

### ECS
- **EC2 Launch Type**: You manage the EC2 infrastructure
- **Fargate Launch Type**: Serverless, just define CPU/memory needs
- **Use Cases**: Host custom inference servers, run batch jobs (preprocessing, feature engineering)
- **Integration**: Can be used with SageMaker or standalone

### EKS
- Managed Kubernetes service on AWS
- Ideal for advanced ML pipelines using tools like **Kubeflow**, **Spark on K8s**
- **Fargate on EKS**: Lets you run pods without managing worker nodes





### ECR
A fully managed Docker container registry to store, version, and secure container images.
- **IAM-secured access**
- **Tagging** (e.g., `model:v1`, `model:latest`)
- **Scanning for vulnerabilities**

**Typical Workflow:**
1. Build image:  
   `docker build -t my-model:v1 .`
2. Authenticate to ECR:  
   `aws ecr get-login-password | docker login ...`
3. Push to ECR:  
   `docker push <account_id>.dkr.ecr.<region>.amazonaws.com/my-model:v1`
---
## 3.7.3 Sample Dockerfile for ML Inference

```dockerfile
FROM python:3.9
RUN pip install boto3 sagemaker scikit-learn
COPY inference.py /app/inference.py
ENTRYPOINT [ "python", "/app/inference.py" ]
```

- **Local Test**:  
  `docker run -p 8080:8080 my-model:v1`

- **Security Tips:**
  - Avoid hardcoding secrets in Docker images
  - Use AWS Secrets Manager or ECS/EKS environment variables
  - Regularly update base images
---

## 3.7.4 Container-Based Deployment Patterns

| Pattern              | Description |
|----------------------|-------------|
| **Single-Model Container** | One model per container; easier management, higher cost at scale |
| **Multi-Model Container**  | Multiple models per container; routes requests internally |
| **Sidecar Container**      | Run support containers (e.g., logging, monitoring) alongside model container |
---

## 3.7.5 Choosing Appropriate Containers

Containerization in ML ensures reliable, consistent, and cost-efficient deployment. The two main approaches are:

### 1. Pre-built Containers

SageMaker provides pre-configured containers for popular ML frameworks:
- **TensorFlow**
- **PyTorch**
- **Scikit-learn**
- **XGBoost**

**Features:**
- Optimized for AWS infrastructure
- Includes extra Python packages (e.g., Pandas, NumPy)
- Supports custom requirements via `requirements.txt`

**Script Mode:**
- Run your own training/inference scripts with minimal changes
- Modularize code across multiple files

**BYOC (Bring Your Own Container):**
- Build a Docker image with your framework/dependencies
- Push to ECR, reference in SageMaker for training/deployment

**When to Use:**
- Standard model architectures
- Rapid prototyping (hackathons, demos)
- Tight deadlines (speed of deployment)

**Advantages:**
- Fast deployment
- Simple setup
- Reliable AWS compatibility


### 2. Customized Containers

Custom containers give you full control over the runtime environment:
- Add your own dependencies, scripts, configurations
- Fit niche frameworks or regulatory requirements

**When to Use:**
- Proprietary or private pre-trained models
- Non-standard/niche ML frameworks
- Compliance/security needs (e.g., financial regulations)

**Advantages:**
- Maximum flexibility
- Tailored security
- Exact dependency management
---
## 3.7.6 Containerization for Edge Devices with SageMaker Neo

**SageMaker Neo** optimizes and compiles ML models for **edge deployment**.

### Benefits:
- Lower latency for inference
- Reduced memory usage
- Improved efficiency on limited-resource devices

### How to Deploy with SageMaker Neo
1. **Train the Model** (regular SageMaker training jobs)
2. **Compile with Neo** (optimize for target hardware, e.g., ARM, Raspberry Pi)
3. **Deploy with Edge Manager** (securely deploy/manage model at the edge)

### Best Practices for Edge Optimization

- **Use Lightweight Models:** Choose MobileNet, EfficientNet-lite
- **Quantization:** Reduce model size by using lower-precision data types
- **Model Pruning:** Remove redundant weights or neurons
- **Monitor Continuously:** Track resource usage, latency, and accuracy
---
## 3.7.7 Multi-Model vs Multi-Container Deployment

### Multi-Model Endpoint (MME)
- **Use Case**: Many models, rarely accessed
- **Advantages**: Cost-effective (shared resources), simple to manage
- **Challenges**: Cold start latency, shared performance limits
- **Example**: Restaurant chain in Manila with multiple menu models

### Multi-Container Endpoint
- **Use Case**: Models with different hardware/resource needs
- **Advantages**: Full isolation, adaptability, scalability
- **Challenges**: More costly and complex
- **Example**: Ayala Corp uses GPU for vision, CPU for stats models

| Strategy             | Cost         | Performance     | Best Fit                                        |
|----------------------|--------------|------------------|--------------------------------------------------|
| Multi-Model Endpoint | Low          | Varies (cold starts) | Infrequent usage, lightweight models             |
| Multi-Container      | High         | High              | High-demand models with varied requirements      |
