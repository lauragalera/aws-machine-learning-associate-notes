
# Amazon SageMaker AI

Amazon SageMaker AI is the next generation of Amazon SageMaker, combining data analytics and artificial intelligence. It supports **real-time**, **batch**, and **asynchronous inference**, offering flexible model deployment options. It also enables **full orchestration of ML workflows**, particularly useful in model deployment and CI/CD operations.

# Amazon SageMaker Auto Scaling

SageMaker supports **Auto Scaling** for deployed models, enabling automatic adjustment of compute instances based on traffic volume.

**Key Features:**

- Automatically provisions more instances during high traffic.
- Scales down during low demand to save costs.
- Enables consistent, performant deployments without manual intervention.

# Amazon SageMaker Data Wrangler

Amazon SageMaker Data Wrangler simplifies the **data preparation phase** of ML workflows, making it easier to import, transform, and engineer data for training.

**Key Contributions in ML Deployment & Orchestration:**

- **Automated Data Preparation**
  - Import, clean, and transform data with low or no code.
  - Integrates with Amazon S3, AWS Glue, Redshift, Athena, and Snowflake.
  - Outputs can be directly used in SageMaker training jobs.

- **Feature Engineering and Selection**
  - Over 300+ built-in transformations.
  - Supports one-hot encoding, binning, normalization, and scaling.
  - Outputs can be sent to **SageMaker Feature Store**.

- **Integration with SageMaker Pipelines**
  - Data Wrangler steps are exportable to pipelines.
  - Enables automation of the entire ML lifecycle.

- **CI/CD Support**
  - Integrates with AWS CodePipeline, AWS Step Functions, and Apache Airflow.

- **Model Monitoring & Data Drift Detection**
  - Can preprocess production-grade data and feed models continuously.

# Amazon SageMaker Feature Store

Amazon SageMaker Feature Store is a **centralized repository** for storing, retrieving, and sharing ML features. It plays a critical role in **streamlining feature reuse and ensuring consistency** across training and inference.

**Key Contributions in ML Deployment & Orchestration:**

- **Centralized Feature Management**
  - Stores features with metadata and timestamps.
  - Reusable across multiple models and pipelines.

- **Online and Offline Feature Stores**
  - **Online Store**: Low-latency access for real-time inference.
  - **Offline Store**: Stores batch data used during training.

- **Integration with Pipelines**
  - Serves as a pipeline step with transformation logic.
  - Supports feature engineering automation.

- **CI/CD Support**
  - Integrates with CodePipeline, Step Functions, and Airflow for full automation.

- **Real-Time Model Predictions**
  - Reduces latency by using precomputed feature values.

- **Feature Drift Monitoring**
  - Detects drift using integration with SageMaker Model Monitor.

**Benefits:**
- Centralized feature control with Redis (online) and S3 (offline).
- Feature consistency across environments.
- Tagging and indexing of features for better discovery.
- Streaming and batch data ingestion.
- Integration with AWS Lake Formation for security and governance.
- Cross-account sharing for better MLOps practices.

# Amazon SageMaker ModelExplainabilityMonitor

Ensures **trust and transparency** by monitoring and explaining ML model predictions.

## Key Features:

- **Explainability Reports with SHAP Values**
  - Identifies which features influenced predictions.
  - Uses SHAP to quantify feature impact.

- **Automated Monitoring**
  - Begins after model deployment to SageMaker Endpoints.
  - Produces ongoing reports and alerts.

- **Integration with Pipelines**
  - Generates reports as a pipeline step.
  - Triggers alerts or retraining on feature drift.

- **Compliance and Auditing**
  - Supports GDPR, Fair Lending, and similar regulations.

- **Model Monitor Integration**
  - Works with other monitors (data quality, model quality, bias).

### Solution Idea:

- Uses **ModelExplainabilityMonitor** to evaluate which features (e.g., post content, metadata) influenced predictions.
- Establishes a baseline and returns drift logs hourly.
- Logs are stored in Amazon S3 for traceability and auditing.

# Amazon SageMaker Neo

Amazon SageMaker Neo converts machine learning models into improved versions that work efficiently on several device endpoints. The system automatically makes changes to each model and adjusts it to boost prediction speed without manual input. This approach fixes deployment plans that require efficiency and low cost.

## Key Features and Functions

- **Model Optimization for Hardware Platforms**  
  - Neo changes models to work better on specific devices (CPUs, GPUs along with Edge units).  
  - It accepts ML frameworks such as TensorFlow, PyTorch, MXNet or XGBoost.  
  - Users can deploy models to AWS Inferentia, Intel, NVIDIA, ARM, and several edge systems.

- **Increased Inference Speed and Efficiency**  
  - Neo makes models smaller and less complex.  
  - Reduced size leads to quicker responses and lower resource needs, cutting costs on cloud or edge units.

- **Cross-Platform Deployment**  
  - Models run on many platforms after one optimization.  
  - Supports cloud servers, IoT gadgets, and local systems — no extra changes needed.  
  - Easily fits into CI/CD pipelines.

- **Integration with SageMaker Pipelines and Endpoints**  
  - Neo operates within SageMaker Pipelines to deploy optimized models to endpoints.  
  - Faster inference, especially for real-time applications.

- **Edge Deployment**  
  - Ideal for devices with limited storage and compute, e.g., mobile phones or IoT sensors.  
  - Integrates with AWS services like IoT Greengrass.

# Amazon SageMaker Pipelines

Amazon SageMaker Pipelines is a CI/CD service dedicated to machine learning that controls the complete ML lifecycle — from data prep to model deployment and monitoring.

## Key Features and Functions

- **Workflow Automation**  
  - Defines ML workflows in sequential steps: data preparation, feature engineering, training, evaluation, deployment, and monitoring  
  - Supports conditional execution and branching

- **Integration with Other AWS Services**  
  - **Data Prep**: S3, AWS Glue, Data Wrangler  
  - **Training**: SageMaker built-in or custom algorithms  
  - **Deployment**: To SageMaker endpoints, batch jobs, or edge hardware  
  - **Monitoring**: Via Model Monitor

- **Reusability and Version Control**  
  - Tracks versions and reuses workflow steps  
  - Encourages collaboration and prevents duplication

- **Integration with CI/CD Tools**  
  - Connects with AWS CodePipeline, Step Functions, Jenkins  
  - Auto-updates triggered by changes in data or code

- **Scalability and Resource Management**  
  - Provisions SageMaker resources on demand  
  - Supports parallel task execution

- **Model Lineage Tracking**  
  - Automatically records data versions, configs, and metrics  
  - Useful for audits and re-runs

# Amazon SageMaker Processing Jobs

SageMaker Processing Jobs handle data preparation, feature engineering, evaluation, and post-inference tasks.

## Key Features and Functions

- **Data Preprocessing**  
  - ETL tasks: remove errors, fix missing values, standardize formats  
  - Works with CSV, JSON, Parquet, Avro

- **Custom Scripts for Processing**  
  - Supports Python, R, Spark  
  - No need for manual server setup

- **Batch Data Processing**  
  - Handles large datasets for historical analysis or post-inference review

- **Integration with Pipelines**  
  - Fits into a SageMaker Pipeline  
  - Automates preprocessing, training, and evaluation

- **Resource Scalability**  
  - Auto provisions CPU/GPU resources as needed

- **Integration with AWS Services**  
  - Works with S3, Glue, Redshift, Athena  
  - Outputs can be used for training or batch transform jobs

**Benefits:**
- **Scalability**: Handles all store data in parallel  
- **Automation**: No manual provisioning  
- **Better Insights**: Higher-quality features → improved ML performance

# Amazon SageMaker Serverless Inference

SageMaker Serverless Inference offers automatic scaling for ML models with minimal setup — ideal for apps with unpredictable workloads.

## Key Features and Functions

- **Serverless Model Deployment**  
  - No instance management needed  
  - Great for infrequent or unpredictable traffic

- **Automatic Scaling**  
  - Scales up under load  
  - Scales to zero during inactivity  
  - Pay only for usage

- **Pipeline Integration**  
  - Easily fits into SageMaker Pipelines  
  - Fast deployment after training/testing

- **Low Latency**  
  - Real-time responses suitable for chatbots, recommendation systems

- **Cost Efficiency**  
  - Usage-based billing reduces waste

- **CI/CD Integration**  
  - Works with AWS CodePipeline and external CI/CD tools  
  - Automates model updates

**Benefits:**
- **Easy Deployment**: No need for infra expertise  
- **On-Demand Scaling**: Handles holiday surges  
- **Real-Time Suggestions**: Instant responses  
- **Cost Efficiency**: Pay only when model is used  
- **CI/CD**: Automatic deployment and rollback

# Introduction to Auto Scaling in SageMaker

Amazon SageMaker automatically adjusts infrastructure to match real-time ML inference traffic.

### Auto Scaling Options:
- **Endpoint Auto Scaling**: Dynamically adjusts instance count for real-time endpoints based on:
  - Metrics (e.g., CPUUtilization, InvocationsPerInstance)
  - Scheduled times
- **AWS Application Auto Scaling**: Manages the scaling under the hood
- **Scheduled Scaling**: Set capacity changes at specific times (e.g., business hours in the Philippines)

## Configuring Auto Scaling for SageMaker Endpoints

### Endpoint Basics:
- Choose instance type (e.g., `ml.c5.xlarge`, `ml.g4dn.xlarge`)
- Set:
  - **Min** instance count
  - **Max** instance count
  - Initial instance count

### Steps to Enable Auto Scaling:

1. **Register a Scalable Target**

```bash
aws application-autoscaling register-scalable-target \
 --service-namespace sagemaker \
 --resource-id endpoint/<EndpointName>/variant/<VariantName> \
 --scalable-dimension sagemaker:variant:DesiredInstanceCount \
 --min-capacity 1 \
 --max-capacity 10
```

2. **Attach a Scaling Policy**

- Choose **TargetTrackingScaling** or **StepScaling**
- Example metric: `SageMakerVariantInvocationsPerInstance`
- Recommended: target value of **70%**

3. **Validate by sending traffic**
- Observe dynamic scaling behavior in CloudWatch

### Common Metrics:
| Metric                      | When to Use                             |
|-----------------------------|------------------------------------------|
| CPUUtilization              | CPU-heavy inference tasks               |
| InvocationsPerInstance      | Request-based scaling                   |
| ModelLatency, OverloadRequests | For latency-sensitive workloads         |

## Using Spot Instances for Cost Savings

### Spot in SageMaker

| Use Case            | Description                                  |
|---------------------|----------------------------------------------|
| **Managed Spot Training** | SageMaker uses spare EC2 capacity for training |
| **Custom Spot Inference** | Use ECS/EKS + Spot to simulate real-time inference |

### Best Practices

- **Checkpointing**: Save model state to S3 to resume after interruption
- **Model Partitioning**: Save intermediate results periodically


