
# Amazon SageMaker AI

Amazon SageMaker AI is the next generation of Amazon SageMaker, combining data analytics and artificial intelligence. It supports **real-time**, **batch**, and **asynchronous inference**, offering flexible model deployment options. It also enables **full orchestration of ML workflows**, particularly useful in model deployment and CI/CD operations.

# Amazon SageMaker Auto Scaling

SageMaker supports **Auto Scaling** for deployed models, enabling automatic adjustment of compute instances based on traffic volume.

 Enabling Auto Scaling on the SageMaker endpoint is the best approach to improve both latency and throughput without significant downtime. Auto Scaling dynamically adjusts the number of instances serving the model based on real-time traffic, which allows the system to handle traffic spikes efficiently by provisioning additional instances when needed and scaling down during low demand. This dynamic scaling ensures that the endpoint maintains low latency and high throughput consistently, optimizing resource utilization and cost.

**Key Features:**

- Automatically provisions more instances during high traffic.
- Scales down during low demand to save costs.
- Enables consistent, performant deployments without manual intervention.

# Amazon SageMaker Data Wrangler

Amazon SageMaker Data Wrangler simplifies the **data preparation phase** of ML workflows, making it easier to import, transform, and engineer data for training.

**Key Contributions in ML Deployment & Orchestration:**

- **Automated Data Preparation**
  - Import, clean, and transform data with low or no code.
  - Integrates with Amazon S3, AWS Glue, Redshift, Athena, and Snowflake.
  - Outputs can be directly used in SageMaker training jobs.

- **Feature Engineering and Selection**
  - Over 300+ built-in transformations.
  - Supports one-hot encoding, binning, normalization, and scaling.
  - Outputs can be sent to **SageMaker Feature Store**.

- **Integration with SageMaker Pipelines**
  - Data Wrangler steps are exportable to pipelines.
  - Enables automation of the entire ML lifecycle.

- **CI/CD Support**
  - Integrates with AWS CodePipeline, AWS Step Functions, and Apache Airflow.

- **Model Monitoring & Data Drift Detection**
  - Can preprocess production-grade data and feed models continuously.

# Amazon SageMaker Feature Store

Amazon SageMaker Feature Store is a **centralized repository** for storing, retrieving, and sharing ML features. It plays a critical role in **streamlining feature reuse and ensuring consistency** across training and inference.

**Key Contributions in ML Deployment & Orchestration:**

- **Centralized Feature Management**
  - Stores features with metadata and timestamps.
  - Reusable across multiple models and pipelines.

- **Online and Offline Feature Stores**
  - **Online Store**: Low-latency access for real-time inference.
  - **Offline Store**: Stores batch data used during training.

Amazon SageMaker Feature Store is designed with two complementary storage options: the online store for low-latency, real-time access during inference, and the offline store for scalable, cost-effective batch training. Using the offline store for training data allows efficient handling of large datasets without incurring the higher costs and latency associated with the online store. Conversely, the online store provides fast retrieval of features needed for real-time predictions. This separation aligns with AWS best practices by optimizing for both performance and cost, ensuring that the recommendation system can serve personalized results quickly while maintaining efficient training workflows.

- **Integration with Pipelines**
  - Serves as a pipeline step with transformation logic.
  - Supports feature engineering automation.

- **CI/CD Support**
  - Integrates with CodePipeline, Step Functions, and Airflow for full automation.

- **Real-Time Model Predictions**
  - Reduces latency by using precomputed feature values.

- **Feature Drift Monitoring**
  - Detects drift using integration with SageMaker Model Monitor.

**Benefits:**
- Centralized feature control with Redis (online) and S3 (offline).
- Feature consistency across environments.
- Tagging and indexing of features for better discovery.
- Streaming and batch data ingestion.
- Integration with AWS Lake Formation for security and governance.
- Cross-account sharing for better MLOps practices.

# SageMaker Profiler

Amazon SageMaker Profiler is a powerful tool that helps you analyze and optimize the performance of your machine learning training jobs. It gives you visibility into how your training code interacts with system resources, such as CPUs, GPUs, memory, network, and disk I/O.

You need to configure the SageMaker Estimator with profiling settings to capture CPU and GPU utilization data at specific intervals during the training job. This allows for effective monitoring and optimization of resource usage.

To track and monitor system resource usage, the SageMaker Profiler modules need to be imported, and start/stop profiling commands must be incorporated into the training script. This ensures that the profiler is actively capturing performance data during training.

Monitoring GPU utilization in real-time helps identify whether the GPUs are underutilized, which could slow down training. This allows the engineer to optimize GPU usage.

To track system resources, SageMaker Profiler requires the start and stop profiling commands to be added to the training script, which collects metrics such as CPU, GPU, and memory usage.

# Amazon SageMaker ModelExplainabilityMonitor

Ensures **trust and transparency** by monitoring and explaining ML model predictions.

## Key Features:

- **Explainability Reports with SHAP Values**
  - Identifies which features influenced predictions.
  - Uses SHAP to quantify feature impact.

- **Automated Monitoring**
  - Begins after model deployment to SageMaker Endpoints.
  - Produces ongoing reports and alerts.

- **Integration with Pipelines**
  - Generates reports as a pipeline step.
  - Triggers alerts or retraining on feature drift.

- **Compliance and Auditing**
  - Supports GDPR, Fair Lending, and similar regulations.

- **Model Monitor Integration**
  - Works with other monitors (data quality, model quality, bias).

### Solution Idea:

- Uses **ModelExplainabilityMonitor** to evaluate which features (e.g., post content, metadata) influenced predictions.
- Establishes a baseline and returns drift logs hourly.
- Logs are stored in Amazon S3 for traceability and auditing.

# Amazon SageMaker Neo

Amazon SageMaker Neo converts machine learning models into improved versions that work efficiently on several device endpoints. The system automatically makes changes to each model and adjusts it to boost prediction speed without manual input. This approach fixes deployment plans that require efficiency and low cost.

After training the model in SageMaker, the data scientist can use SageMaker Neo to compile the model for the specific hardware (such as IoT or edge devices) and deploy it using AWS IoT Greengrass, ensuring optimized performance.

## Key Features and Functions

- **Model Optimization for Hardware Platforms**  
  - Neo changes models to work better on specific devices (CPUs, GPUs along with Edge units).  
  - It accepts ML frameworks such as TensorFlow, PyTorch, MXNet or XGBoost.  
  - Users can deploy models to AWS Inferentia, Intel, NVIDIA, ARM, and several edge systems.

- **Increased Inference Speed and Efficiency**  
  - Neo makes models smaller and less complex.  
  - Reduced size leads to quicker responses and lower resource needs, cutting costs on cloud or edge units.

- **Cross-Platform Deployment**  
  - Models run on many platforms after one optimization.  
  - Supports cloud servers, IoT gadgets, and local systems — no extra changes needed.  
  - Easily fits into CI/CD pipelines.

- **Integration with SageMaker Pipelines and Endpoints**  
  - Neo operates within SageMaker Pipelines to deploy optimized models to endpoints.  
  - Faster inference, especially for real-time applications.

- **Edge Deployment**  
  - Ideal for devices with limited storage and compute, e.g., mobile phones or IoT sensors.  
  - Integrates with AWS services like IoT Greengrass.

# Amazon SageMaker Pipelines

Amazon SageMaker Pipelines is a CI/CD service dedicated to machine learning that controls the complete ML lifecycle — from data prep to model deployment and monitoring.

SageMaker Pipelines callback steps are specifically designed to integrate external processes into the SageMaker pipeline workflow. By using a callback step, the SageMaker pipeline waits until the AWS Glue jobs complete. The output of the AWS Glue jobs, stored in Amazon S3, is then passed to subsequent steps in the pipeline. This approach eliminates the need for custom orchestration scripts, manual intervention, or redundant scheduling, ensuring minimal operational overhead. Callback steps are an efficient way to synchronize external workflows, like AWS Glue, with SageMaker Pipelines.

Amazon SageMaker Pipelines is the recommended solution for implementing manual approval-based workflows for model deployment. SageMaker Pipelines allows you to design automated ML workflows, including steps for training, registering models in SageMaker Model Registry, and deploying models to endpoints. You can use conditional steps in SageMaker Pipelines to introduce a manual approval step before proceeding to production deployments. This ensures only models explicitly approved by a human reviewer are deployed, which aligns perfectly with the requirement for governance and control.

Pipeline parameters allow the user to introduce variables that can be overridden when executing the pipeline, providing flexibility without needing to modify the pipeline itself.

SageMaker Pipelines use a directed acyclic graph (DAG) to define the sequence and relationships of steps. Parameters in the pipeline enable reusable, customizable workflows, improving automation and flexibility.

## Key Features and Functions

- **Workflow Automation**  
  - Defines ML workflows in sequential steps: data preparation, feature engineering, training, evaluation, deployment, and monitoring  
  - Supports conditional execution and branching

- **Integration with Other AWS Services**  
  - **Data Prep**: S3, AWS Glue, Data Wrangler  
  - **Training**: SageMaker built-in or custom algorithms  
  - **Deployment**: To SageMaker endpoints, batch jobs, or edge hardware  
  - **Monitoring**: Via Model Monitor

- **Reusability and Version Control**  
  - Tracks versions and reuses workflow steps  
  - Encourages collaboration and prevents duplication

- **Integration with CI/CD Tools**  
  - Connects with AWS CodePipeline, Step Functions, Jenkins  
  - Auto-updates triggered by changes in data or code

- **Scalability and Resource Management**  
  - Provisions SageMaker resources on demand  
  - Supports parallel task execution

- **Model Lineage Tracking**  
  - Automatically records data versions, configs, and metrics  
  - Useful for audits and re-runs

Model Lineage tracks the history of model development, including data used, pre-processing steps, and training runs, making it crucial for auditing and compliance.

# Amazon SageMaker Processing Jobs

SageMaker Processing Jobs handle data preparation, feature engineering, evaluation, and post-inference tasks.

## Key Features and Functions

- **Data Preprocessing**  
  - ETL tasks: remove errors, fix missing values, standardize formats  
  - Works with CSV, JSON, Parquet, Avro

- **Custom Scripts for Processing**  
  - Supports Python, R, Spark  
  - No need for manual server setup

- **Batch Data Processing**  
  - Handles large datasets for historical analysis or post-inference review

- **Integration with Pipelines**  
  - Fits into a SageMaker Pipeline  
  - Automates preprocessing, training, and evaluation

- **Resource Scalability**  
  - Auto provisions CPU/GPU resources as needed

- **Integration with AWS Services**  
  - Works with S3, Glue, Redshift, Athena  
  - Outputs can be used for training or batch transform jobs

**Benefits:**
- **Scalability**: Handles all store data in parallel  
- **Automation**: No manual provisioning  
- **Better Insights**: Higher-quality features → improved ML performance

# Amazon SageMaker Serverless Inference

Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure. On-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers.

SageMaker AI manages all of the underlying infrastructure, so there’s no need to manage instances or scaling policies. You pay only for what you use and not for idle time. It can support payload sizes up to 4 MB and processing times up to 60 seconds. Setting MaxConcurrency to 1 ensures the endpoint can handle one request at a time, which is sufficient for the daily batch inference.
## Key Features and Functions

- **Serverless Model Deployment**  
  - No instance management needed  
  - Great for infrequent or unpredictable traffic

- **Automatic Scaling**  
  - Scales up under load  
  - Scales to zero during inactivity  
  - Pay only for usage

- **Pipeline Integration**  
  - Easily fits into SageMaker Pipelines  
  - Fast deployment after training/testing

- **Low Latency**  
  - Real-time responses suitable for chatbots, recommendation systems

- **Cost Efficiency**  
  - Usage-based billing reduces waste

- **CI/CD Integration**  
  - Works with AWS CodePipeline and external CI/CD tools  
  - Automates model updates

**Benefits:**
- **Easy Deployment**: No need for infra expertise  
- **On-Demand Scaling**: Handles holiday surges  
- **Real-Time Suggestions**: Instant responses  
- **Cost Efficiency**: Pay only when model is used  
- **CI/CD**: Automatic deployment and rollback

# Introduction to Auto Scaling in SageMaker

Amazon SageMaker automatically adjusts infrastructure to match real-time ML inference traffic.

### Auto Scaling Options:
- **Endpoint Auto Scaling**: Dynamically adjusts instance count for real-time endpoints based on:
  - Metrics (e.g., CPUUtilization, InvocationsPerInstance)
  - Scheduled times
- **AWS Application Auto Scaling**: Manages the scaling under the hood
- **Scheduled Scaling**: Set capacity changes at specific times (e.g., business hours in the Philippines)

## Configuring Auto Scaling for SageMaker Endpoints

### Endpoint Basics:
- Choose instance type (e.g., `ml.c5.xlarge`, `ml.g4dn.xlarge`)
- Set:
  - **Min** instance count
  - **Max** instance count
  - Initial instance count

### Steps to Enable Auto Scaling:

1. **Register a Scalable Target**

```bash
aws application-autoscaling register-scalable-target \
 --service-namespace sagemaker \
 --resource-id endpoint/<EndpointName>/variant/<VariantName> \
 --scalable-dimension sagemaker:variant:DesiredInstanceCount \
 --min-capacity 1 \
 --max-capacity 10
```

2. **Attach a Scaling Policy**

- Choose **TargetTrackingScaling** or **StepScaling**
- Example metric: `SageMakerVariantInvocationsPerInstance`
- Recommended: target value of **70%**

3. **Validate by sending traffic**
- Observe dynamic scaling behavior in CloudWatch

### Common Metrics:
| Metric                      | When to Use                             |
|-----------------------------|------------------------------------------|
| CPUUtilization              | CPU-heavy inference tasks               |
| InvocationsPerInstance      | Request-based scaling                   |
| ModelLatency, OverloadRequests | For latency-sensitive workloads         |

## SageMaker Notebook Instances

SageMaker Notebook Instances provide fully managed Jupyter notebooks that simplify machine learning workflows. These notebooks automatically handle infrastructure setup, including server configuration, and come with popular ML libraries like TensorFlow and PyTorch pre-installed. They also offer persistent storage, allowing work to be saved and accessed later.

SageMaker Notebook Instances are standalone, fully managed Jupyter notebooks running on EC2 instances. They’re good for lightweight, script-based ML workflows.

SageMaker Notebook Instances offer persistent storage, ensuring that data, code, and results are saved even after the notebook instance is stopped. This eliminates the risk of data loss during development.

## SageMaker Studio

SageMaker Studio offers a collaborative environment where multiple users can manage and share notebooks. It provides integrated support for scaling resources (e.g., adding GPUs) and allows seamless transitions between data preparation, model training, and deployment—all within a unified interface.

Studio offers a richer, more integrated experience for serious ML workflows and team environments than SM Notebook Instances.

SageMaker Studio provides a graphical interface where the team can visualize and compare key performance metrics (e.g., accuracy, precision) across different trials. This makes it easy to select the best performing model configuration.

## Using Spot Instances for Cost Savings

EC2 Spot Instances are the best fit for:

- Leverage unused EC2 capacity at significantly reduced prices, offering up to 90% cost savings compared to On-Demand pricing.

- Run the job in off-peak hours (Saturday night), reducing the likelihood of interruptions.

- Benefit from the fact that the job tolerates interruptions, aligning perfectly with Spot Instances' use case.

This makes Spot Instances the most cost-effective solution for a short-duration, intermittent, and interruption-tolerant workload.

### Spot in SageMaker

#### Managed Spot Training

Managed Spot Training uses Amazon EC2 Spot instance to run training jobs instead of on-demand instances. You can specify which training jobs use spot instances and a stopping condition that specifies how long SageMaker waits for a job to run using Amazon EC2 Spot instances. Spot instances can be interrupted, causing jobs to take longer to start or finish. You can configure your managed spot training job to use checkpoints. SageMaker copies checkpoint data from a local path to Amazon S3. When the job is restarted, SageMaker copies the data from Amazon S3 back into the local path. The training job can then resume from the last checkpoint instead of restarting.

Amazon EC2 Auto Scaling can add Spot Instances based on demand, but it does not provide the same level of automation and resilience as SageMaker Managed Spot Training

Example: The training job can tolerate interruptions and is expected to run for several hours or even days, depending on the available compute resources. The company has a limited budget for cloud infrastructure, so you need to minimize costs as much as possible.

| Use Case            | Description                                  |
|---------------------|----------------------------------------------|
| **Managed Spot Training** | SageMaker uses spare EC2 capacity for training |
| **Custom Spot Inference** | Use ECS/EKS + Spot to simulate real-time inference |

### Best Practices

- **Checkpointing**: Save model state to S3 to resume after interruption
- **Model Partitioning**: Save intermediate results periodically


