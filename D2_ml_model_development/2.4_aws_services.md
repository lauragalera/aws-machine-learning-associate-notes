# Using AWS AI Services to Solve Business Problems

## Amazon Translate: Language Translation
A **fully managed neural machine translation** service that translates text in real-time.

 Amazon Translate's asynchronous batch translation is optimized for large datasets, providing cost-effective, scalable, and efficient processing suitable for thousands of reviews daily.

## Amazon Polly: Text-to-Speech Service
Uses ML to convert text into natural-sounding speech, including:

- Multiple lifelike voices and languages
- Real-time streaming or file generation
- Support for speech marks and SSML for customization

The best approach for producing lifelike speech with customized pronunciations and handling high volumes with minimal latency is to use **Amazon Polly Neural TTS voices combined with a custom lexicon**. Neural TTS voices provide superior naturalness and expressiveness in speech synthesis, which is essential for accessibility features. Custom lexicons enable precise control over the pronunciation of specific words and phrases, ensuring they are spoken correctly and naturally. Standard voices and SSML alone do not offer the same level of naturalness or flexibility as Neural TTS voices with custom lexicons.

Converting numbers to their textual equivalents helps Amazon Polly pronounce numbers correctly and avoid misinterpretations, which is a critical step in data preparation for clear speech output.

Validating the punctuation and grammar of input text to ensure natural and correct pronunciation in speech output.

## Amazon Transcribe: Speech-to-Text Applications
An AI service that **converts speech to text**, offering:
- Real-time transcription  
- Speaker identification  
- Custom vocabularies  

Custom vocabularies enhance transcription accuracy by including industry-specific terms, and batch transcription jobs allow efficient processing of large volumes of audio files asynchronously.

## Amazon Textract: Document Text and Data Extraction
Uses ML to automatically extract text, forms, and tables from scanned documents and images, including:

- Printed and handwritten text extraction
- Form data and key-value pair recognition
- Table detection and structured data extraction

The AnalyzeDocument API is designed to extract structured data such as key-value pairs and tables, making it ideal for populating databases. Combining it with Amazon Comprehend enhances text analysis for legal use cases, including entity recognition and classification.

Amazon Textract is perfect for extracting text from scanned documents.

## Amazon Comprehend: Natural Language Processing
Uses ML to extract insights from text, including:

- Sentiment analysis
- Entity recognition (people, places, dates, etc.)
- Key phrase extraction and topic modeling

Enable Amazon Comprehend's built-in data logging and monitoring features to track access and usage.

Apply encryption both in transit and at rest for all data processed and stored.

## Amazon Lookout for Metrics: Automated Anomaly Detection
Uses ML to detect anomalies in business and operational data, including:

- Monitoring metrics across applications and services
- Identifying unusual trends or outliers automatically
- Reducing manual data analysis effort

## Amazon Lookout for Vision: Automated Visual Defect Detection
Uses ML to detect anomalies and defects in images, including:

- Quality inspection for manufacturing
- Identifying damaged or missing components
- No ML expertise required for training models

## Amazon DevOps Guru: ML-Powered Operational Insights
Uses ML to detect and diagnose application issues, including:

- Identifying performance bottlenecks and anomalies
- Root cause analysis with recommendations
- Reducing mean time to resolution (MTTR) automatically

Amazon DevOps Guru helps detect unusual application behavior, such as memory leaks or high response times, which can indirectly affect the performance of ML models deployed within those environments.

## Amazon Rekognition: Image and Video Analysis
Uses ML to **analyze images/videos**, including:
- Object detection  
- Facial recognition  
- Content moderation 

Amazon Rekognition Face Search enables real-time searching and comparison of faces against a collection, such as a watchlist, making it ideal for identifying unauthorized individuals in a security surveillance system.

## Amazon Bedrock: Foundation Models for NLP & Image Generation
Provides access to **foundation models (FMs)** without managing infrastructure. Supports:
- Chatbots  
- Text generation  
- Image generation 

Amazon Bedrock offers a serverless environment that automatically scales based on the workload, making it suitable for applications that require dynamic scalability without manual intervention.

Bedrock ensures that data is encrypted in transit and at rest, providing secure integration of generative AI models without compromising data security.

Knowledge bases in Amazon Bedrock can be used to incorporate additional private data, making the model responses more personalized and relevant for the recommendation engine.

Agents in Amazon Bedrock can help by breaking down complex tasks and connecting to APIs for real-time data, improving the accuracy of the recommendation system by integrating up-to-date information.

Amazon Bedrock offers a provisioned throughput mode, which is useful for large, steady workloads, ensuring reliable performance by allowing users to purchase throughput units for token processing.

Bedrock operates in a serverless environment, which handles automatic scaling and infrastructure management. Additionally, it ensures data encryption both at rest and in transit.


Glossary:

- **Temperature parameter:** A setting in language models that controls the randomness of token selection during generation; lower values produce more deterministic and consistent outputs.

- **Top-K parameter:** A parameter that limits the number of candidate tokens considered at each step in generation, reducing variability and promoting more consistent responses.

- **Caching:** Storing previously generated responses to reuse them later, which can improve response time and consistency for repeated queries. 

Imagine you have deployed a LLM but for the same question it is giving different answers. What can you do? To achieve consistent responses from a chatbot powered by an LLM in Amazon Bedrock, it is essential to reduce the inherent randomness in the model's output generation. This is effectively done by **lowering the temperature and decreasing the top-K parameter**, which constrains the model’s token selection to fewer, more probable options, resulting in more deterministic and repeatable answers. Furthermore, fine-tuning the LLM with **retail-specific datasets aligns the model’s knowledge with the company’s domain**, enhancing its ability to provide accurate and consistent responses related to product information and return policies. Other approaches like caching or monitoring logs do not fundamentally address the variability in the model’s generated outputs.

Amazon Bedrock automatically manages scaling and resource allocation internally without requiring manual configuration of AWS Auto Scaling.

Ensuring data compliance typically involves validating and preprocessing data outside of Amazon Bedrock using dedicated tools or AWS services to meet regulatory requirements before deployment.

## SageMaker AutoPilot: No-code ML Model Builder
A **no-code solution** for automated model training, tuning, and deployment.

## SageMaker JumpStart: Quick Start for ML Models
Provides **pre-trained open-source models** to accelerate development.

## Summary Table

| Service               | Purpose                              |
|------------------------|---------------------------------------|
| Amazon Translate       | Real-time language translation        |
| Amazon Transcribe      | Speech to text                        |
| Amazon Rekognition     | Image and video analysis              |
| Amazon Bedrock         | Access to foundation models (FMs)     |
| SageMaker AutoPilot    | No-code automated model creation      |
| SageMaker JumpStart    | Use pretrained models instantly       |

# Using SageMaker Canvas

Amazon SageMaker Canvas gives you the ability to use machine learning to generate predictions without needing to write any code. The following are some use cases where you can use SageMaker Canvas:

- Predict customer churn

- Plan inventory efficiently

- Optimize price and revenue

- Improve on-time deliveries

- Classify text or images based on custom categories

- Identify objects and text in images

- Extract information from documents

To bring your model into SageMaker Canvas, you need to meet the following requirements:

- You must have a Amazon SageMaker Studio Classic user who has onboarded to Amazon SageMaker AI domain. The Studio Classic user must be in the same domain as the Canvas user.

- For any model that you’ve built outside of SageMaker AI, you must register your model in Model Registry before importing it into Canvas.

- The Canvas user with whom you want to share your model must have permission to access the Amazon S3 bucket in which you store your datasets and model artifacts.

# Using SageMaker Built-In Algorithms

Amazon SageMaker provides a suite of **built-in, high-performance algorithms** designed for various machine learning tasks. These are optimized for speed and scalability, requiring minimal setup.

## Overview of SageMaker Built-In Algorithms

- **Ready-to-use**: No need to build from scratch
- **Highly optimized**: Pre-tuned for performance and scalability
- **Ideal for common tasks** such as:
  - Classification
  - Regression
  - Clustering
  - Anomaly detection

## Common Built-In Algorithms

### 1. **XGBoost**
- **Type**: Supervised learning
- **Best For**: Classification and regression
- **How It Works**: Implements gradient boosted decision trees for accurate predictions

### 2. **Linear Learner**
- **Type**: Supervised learning
- **Best For**: Binary classification, multi-class classification, regression
- **How It Works**: Applies linear models (logistic or linear regression), with automatic model tuning

### 3. **K-Means Clustering**
- **Type**: Unsupervised learning
- **Best For**: Data segmentation
- **How It Works**: Groups data into *k* clusters based on similarity

## Summary Table

| Algorithm         | Type            | Use Case                          |
|------------------|------------------|-----------------------------------|
| XGBoost           | Supervised       | Fraud detection, classification   |
| Linear Learner    | Supervised       | Price prediction, classification  |
| K-Means Clustering| Unsupervised     | Customer segmentation             |

# Integrating Models Built Outside SageMaker

## Importing Pre-built Models into SageMaker

- Amazon SageMaker supports importing and deploying pre-trained models built outside SageMaker.
- Compatible frameworks include TensorFlow, PyTorch, and Scikit-learn.
- Enables faster inference and scalability by leveraging SageMaker’s managed infrastructure.

## Fine-tuning Pre-trained Models with Custom Datasets

- Fine-tuning involves training a pre-trained model further using your own dataset to improve performance on specific tasks.
- SageMaker JumpStart and Amazon Bedrock provide pre-built foundation models ready for fine-tuning.
- Fine-tuning helps customize models for local contexts or specialized use cases.

 The best solution for fine-tuning a model with low-code/no-code requirements involves using Amazon SageMaker JumpStart, Canvas, and Data Wrangler. SageMaker JumpStart provides pre-built LLMs and fine-tuning templates that drastically reduce the complexity of model customization. SageMaker Canvas offers a no-code interface that empowers users without deep ML expertise to participate in model building and fine-tuning. Data Wrangler facilitates efficient data preparation and visualization, which are essential steps before fine-tuning, ensuring high-quality input data. Together, these services provide a scalable, streamlined, and fast deployment pipeline for fine-tuning LLMs on Amazon SageMaker, aligning perfectly with the company’s requirements. 

## Key Points

- SageMaker enables seamless import and deployment of models built externally.
- Fine-tuning with custom data enhances model relevance and accuracy.
- Use SageMaker JumpStart and Amazon Bedrock for easy access to foundation models.

# Managing Model Versions and Repeatability

## Importance of Versioning and Repeatability  
Ensures consistent model performance in production, enables rollback, and supports auditing.

## Amazon SageMaker Model Registry  
A managed service for ML model version control and governance.

### Key Features
- **Model Versioning:** Track and rollback to previous model versions.
- **Approval Workflow:** Approve or reject models before deployment.
- **Metadata Tracking:** Automatically records training jobs, hyperparameters, and artifacts.
- **Auditing:** Logs who deployed what and when.

### Example  
An e-commerce platform trains monthly recommendation models and uses the Model Registry to track and rollback versions if sales conversion drops.

## Managing Model Metadata for Reproducibility and Compliance

### Metadata Includes:
- Training data details
- Hyperparameters used
- Evaluation metrics
- Deployment logs and approvals

### Benefits  
Allows tracing decisions, avoiding unexpected biases or errors, and supports compliance with regulations.

# Using SageMaker Clarify for Model Insights

## What is Amazon SageMaker Clarify?
- A tool to analyze training data and ML model performance.
- Provides insights into model behavior.
- Detects bias in datasets and models.

## Example Use Case
- A job application ML model may prioritize applicants from “big four” universities over skills.
- SageMaker Clarify helps confirm if such bias exists based on school background.

## Identifying and Mitigating Model Bias
- Bias occurs when an ML model unfairly favors or disadvantages certain groups (gender, age, location, etc.).
- Mitigation techniques include:
  - Rebalancing the dataset
  - Fairness-aware training
  - Post-hoc bias correction

## Example
- In a loan approval model, SageMaker Clarify ensures the model does not discriminate against self-employed applicants such as freelancers or sari-sari store owners.
- Actions can then be taken to mitigate detected bias.

# Convergence Issues and Debugging

## What are Convergence Issues?
- Occur when an ML model fails to reach optimal performance during training.
- Causes include:
  - Poor data quality
  - Incorrect hyperparameter settings
  - Inefficient optimization algorithms

## Debugging Process
1. **Issue Identification**  
   - Check model accuracy, loss curves, and training logs.
2. **Data Analysis**  
   - Ensure dataset is clean and balanced.
3. **Hyperparameter Optimization**  
   - Adjust batch size, learning rate, or model architecture.
4. **AWS Tools**  
   - Use Amazon SageMaker Model Debugger for efficient debugging.

## Example
- A fintech startup faces a non-decreasing loss while training a fraud detection model.
- They analyze outliers and rare fraudulent cases to fix convergence problems.

## Using Amazon SageMaker Model Debugger
- Detects training anomalies and inefficiencies in real time.
- Logs and analyzes model parameters (loss, gradients).
- Prevents issues before deployment.

The Debugger hook captures tensors (model states) that provide real-time insights into training progress and issues.

SageMaker Debugger allows the use of predefined or custom debug rules that help identify issues like vanishing gradients and overfitting. Adjusting the learning rate or using different optimizers can help mitigate disappearing gradients.

