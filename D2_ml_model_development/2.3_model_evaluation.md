# Model Evaluation Techniques and Metrics

## Common Evaluation Metrics

### Classification Metrics
- **Accuracy**  
  Proportion of correct predictions out of all predictions.
- **Precision**  
  How many predicted positives are actually positive.
- **Recall**  
  How many actual positives are correctly detected.
- **F1 Score**  
  Harmonic mean of precision and recall; useful for imbalanced datasets.
- **ROC AUC (Receiver Operating Characteristic - Area Under Curve)**  
  Measures how well the model distinguishes between classes.

Precision and recall are particularly important in an imbalanced dataset. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of actual positives that are correctly identified. 

Imagine a model with 85% precision, and 60% recall. The precision is high (85%), indicating the model correctly classifies most of the predicted fraudulent transactions as fraudulent. However, recall is lower (60%), meaning it misses a significant number of actual fraudulent cases.

While accuracy is a common metric, it is not suitable for imbalanced datasets because it can be misleading. 

### Regression Metrics
- **RMSE (Root Mean Squared Error)**  
  Measures the magnitude of prediction errors (lower is better).
- **MAE (Mean Absolute Error)**  
  Measures average absolute difference between actual and predicted values.

## How to Interpret Evaluation Tools

### Confusion Matrix
|                      | Predicted Positive | Predicted Negative |
|----------------------|--------------------|--------------------|
| **Actual Positive**   | True Positive (TP)  | False Negative (FN) |
| **Actual Negative**   | False Positive (FP) | True Negative (TN)  |

- **True Positives (TP):** Correct positive predictions  
- **True Negatives (TN):** Correct negative predictions  
- **False Positives (FP):** Incorrect positive predictions  
- **False Negatives (FN):** Incorrect negative predictions  

The confusion matrix is crucial for understanding the detailed performance of your model, especially in an imbalanced dataset. It allows you to calculate additional metrics such as precision, recall, and F1 score, which are essential for understanding how well your model handles false positives and false negatives.

## Heat Maps
Used to visualize error distribution across categories.

### Example  
In sentiment analysis, heat maps show how well the model classifies sentiments like happy, sad, or neutral.

# Establishing Performance Baselines

## Setting Baseline Performance
- Use simple models or heuristics before training complex ML models.
- Baselines can be statistical measures like mean prediction or rule-based models.
- Purpose: Provides a point of comparison to evaluate the added value of ML models.

Using SageMaker’s linear learner algorithm is an effective approach for creating a simple and interpretable baseline. This method allows you to establish a performance benchmark using key features that are directly related to predicting CLV. The linear learner is quick to train and provides a clear point of comparison for more complex models.

## Comparing Model Performance Against Baseline
- Use metrics like RMSE for regression, Accuracy or F1 Score for classification.
- A model that performs better than the baseline is worth deploying.

# Identifying Overfitting and Underfitting

## Overfitting
- Model performs well on training data but poorly on validation/test data.
- Model memorizes training data and fails to generalize to unseen data.

### Example
- E-commerce churn prediction using only one customer’s historical purchases leads to overfitting.

## Underfitting
- Model performs poorly on both training and validation data.
- Model is too simple to capture underlying patterns.

### Example
- Predicting churn using only a single feature like age results in underfitting.

