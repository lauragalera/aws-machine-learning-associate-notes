# Model Evaluation Techniques and Metrics

## Common Evaluation Metrics

### Classification Metrics
- **Accuracy**  
  Proportion of correct predictions out of all predictions.
- **Precision**  
  How many predicted positives are actually positive.
- **Recall**  
  How many actual positives are correctly detected.
- **F1 Score**  
  Harmonic mean of precision and recall; useful for imbalanced datasets.
- **ROC AUC (Receiver Operating Characteristic - Area Under Curve)**  
  Measures how well the model distinguishes between classes.

### Regression Metrics
- **RMSE (Root Mean Squared Error)**  
  Measures the magnitude of prediction errors (lower is better).
- **MAE (Mean Absolute Error)**  
  Measures average absolute difference between actual and predicted values.

## How to Interpret Evaluation Tools

### Confusion Matrix
|                      | Predicted Positive | Predicted Negative |
|----------------------|--------------------|--------------------|
| **Actual Positive**   | True Positive (TP)  | False Negative (FN) |
| **Actual Negative**   | False Positive (FP) | True Negative (TN)  |

- **True Positives (TP):** Correct positive predictions  
- **True Negatives (TN):** Correct negative predictions  
- **False Positives (FP):** Incorrect positive predictions  
- **False Negatives (FN):** Incorrect negative predictions  

## Heat Maps
Used to visualize error distribution across categories.

### Example  
In sentiment analysis, heat maps show how well the model classifies sentiments like happy, sad, or neutral.

# Establishing Performance Baselines

## Setting Baseline Performance
- Use simple models or heuristics before training complex ML models.
- Baselines can be statistical measures like mean prediction or rule-based models.
- Purpose: Provides a point of comparison to evaluate the added value of ML models.

## Comparing Model Performance Against Baseline
- Use metrics like RMSE for regression, Accuracy or F1 Score for classification.
- A model that performs better than the baseline is worth deploying.

# Identifying Overfitting and Underfitting

## Overfitting
- Model performs well on training data but poorly on validation/test data.
- Model memorizes training data and fails to generalize to unseen data.

### Example
- E-commerce churn prediction using only one customerâ€™s historical purchases leads to overfitting.

## Underfitting
- Model performs poorly on both training and validation data.
- Model is too simple to capture underlying patterns.

### Example
- Predicting churn using only a single feature like age results in underfitting.

