
# 2.3 Model Evaluation Techniques and Metrics

- [2.3.1 Common Evaluation Metrics](#231-common-evaluation-metrics)
  - [Classification Metrics](#classification-metrics)
  - [Regression Metrics](#regression-metrics)
- [2.3.2 How to Interpret Evaluation Tools](#232-how-to-interpret-evaluation-tools)
  - [Confusion Matrix](#confusion-matrix)
  - [Heat Maps](#heat-maps)
- [2.3.3 Establishing Performance Baselines](#233-establishing-performance-baselines)
  - [Setting Baseline Performance](#setting-baseline-performance)
  - [Comparing Model Performance Against Baseline](#comparing-model-performance-against-baseline)
- [2.3.4 Identifying Overfitting and Underfitting](#234-identifying-overfitting-and-underfitting)
  - [Overfitting](#overfitting)
  - [Underfitting](#underfitting)

---

## 2.3.1 Common Evaluation Metrics

### Classification Metrics
- **Accuracy:** Proportion of correct predictions out of all predictions. Not suitable for imbalanced datasets.
- **Precision:** How many predicted positives are actually positive. Important for imbalanced data.
- **Recall:** How many actual positives are correctly detected. Important for imbalanced data.
- **F1 Score:** Harmonic mean of precision and recall; useful for imbalanced datasets.
- **ROC AUC:** Measures how well the model distinguishes between classes.

> **Example:**
> - A model with 85% precision and 60% recall correctly classifies most predicted frauds, but misses many actual frauds.

### Regression Metrics
- **RMSE (Root Mean Squared Error):** Magnitude of prediction errors (lower is better).
- **MAE (Mean Absolute Error):** Average absolute difference between actual and predicted values.

---

## 2.3.2 How to Interpret Evaluation Tools

### Confusion Matrix

|                      | Predicted Positive | Predicted Negative |
|----------------------|--------------------|--------------------|
| **Actual Positive**   | True Positive (TP)  | False Negative (FN) |
| **Actual Negative**   | False Positive (FP) | True Negative (TN)  |

- **True Positives (TP):** Correct positive predictions
- **True Negatives (TN):** Correct negative predictions
- **False Positives (FP):** Incorrect positive predictions
- **False Negatives (FN):** Incorrect negative predictions

> The confusion matrix is crucial for understanding detailed model performance, especially in imbalanced datasets. It enables calculation of precision, recall, and F1 score, and helps you see how well your model handles false positives and false negatives.

### Heat Maps
- Visualize error distribution across categories.
- **Example:** In sentiment analysis, heat maps show how well the model classifies sentiments like happy, sad, or neutral.

---

## 2.3.3 Establishing Performance Baselines

### Setting Baseline Performance
- Use simple models or heuristics before training complex ML models.
- Baselines can be statistical (mean prediction) or rule-based.
- Purpose: Provides a point of comparison to evaluate the added value of ML models.
> **Tip:** SageMaker’s linear learner is a quick, interpretable baseline for regression or classification.

### Comparing Model Performance Against Baseline
- Use metrics like RMSE for regression, Accuracy or F1 Score for classification.
- A model that performs better than the baseline is worth deploying.

---

## 2.3.4 Identifying Overfitting and Underfitting

### Overfitting
- Model performs well on training data but poorly on validation/test data.
- Model memorizes training data and fails to generalize to unseen data.
- **Example:** E-commerce churn prediction using only one customer’s historical purchases leads to overfitting.

### Underfitting
- Model performs poorly on both training and validation data.
- Model is too simple to capture underlying patterns.
- **Example:** Predicting churn using only a single feature like age results in underfitting.

