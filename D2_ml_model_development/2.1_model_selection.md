
# 2.1 Model Selection: Comparing and Choosing the Right Algorithm


- [2.1.1 Common AWS ML Algorithms](#211-common-aws-ml-algorithms)
  - [Random Cut Forest (RCF)](#random-cut-forest-rcf)
  - [DeepAR](#deepar)
  - [Linear Learner](#linear-learner)
  - [Time Series K-Means](#time-series-k-means)
  - [XGBoost](#xgboost)
  - [K-Nearest Neighbors (k-NN)](#k-nearest-neighbors-k-nn)
  - [Factorization Machines](#factorization-machines)
- [2.1.2 Key Considerations When Selecting a Model](#212-key-considerations-when-selecting-a-model)
- [2.1.3 Algorithm Trade-offs](#213-algorithm-trade-offs)
- [2.1.4 Interpretability in Model Selection](#214-interpretability-in-model-selection)
- [2.1.5 Cost Considerations](#215-cost-considerations)
- [2.1.6 Model Ensembling](#216-model-ensembling)

---

## 2.1.1 Common AWS ML Algorithms

No single model is best for all tasks. Each has strengths and trade-offs. Use the guide below to quickly compare common AWS ML algorithms and their best-fit scenarios.

### Random Cut Forest (RCF)
**Best for:** **Anomaly detection** in time series or sensor data
- **Unsupervised:** Finds unusual patterns without labels
- **Great for:** Detecting equipment failure, fraud, or outliers
- **Limitation:** Not ideal for tasks needing labeled data


### DeepAR
**Best for:** **Forecasting future values** in time series
- **Deep learning:** Models complex trends and dependencies
- **Great for:** Predicting sales, demand, or health risks
- **Limitation:** Not designed for anomaly detection


### Linear Learner
**Best for:** **Supervised learning** (classification/regression)
- **Handles class imbalance:** Uses class weights
- **Great for:** Churn prediction, binary classification
- **Simple, fast, and easy to use**
- **Limitation:** May miss complex patterns in data


### Time Series K-Means
**Best for:** **Clustering similar time series patterns**
- **Groups by similarity:** Finds similar time series
- **Limitation:** Clustering alone does not detect anomalies or provide precise predictions


### XGBoost
**Best for:** **Structured/tabular data**, **imbalanced datasets**
- **Powerful gradient boosting algorithm**
- **Great for:** Fraud detection, competitions, optimizing precision/recall
- **Handles class imbalance and overfitting well**
- **Limitation:** Not designed for recommendations or unstructured data


### K-Nearest Neighbors (k-NN)
**Best for:** **Simple classification** based on similarity
- **Easy to understand and implement**
- **Limitation:** Slow on large datasets, struggles with high-dimensional data


### Factorization Machines
**Best for:** **Sparse, high-dimensional data** (e.g., recommendations)
- **Captures feature interactions efficiently**
- **Great for:** Click prediction, collaborative filtering, recommendations

---

## 2.1.2 Key Considerations When Selecting a Model

1. **Understand the Use Case**
   - What problem are you solving? (e.g., classification, regression, anomaly detection)
   - Do you need interpretability or just accuracy?
2. **Data Characteristics**
   - Is your data labeled or unlabeled?
   - Is it balanced or imbalanced?
   - Is it structured (tables) or unstructured (text, images)?
3. **Business Constraints**
   - Do you need fast predictions (real-time)?
   - Are there cost or infrastructure limits?

---

## 2.1.3 Algorithm Trade-offs

### Performance Metrics
- **Accuracy**: % of correct predictions (can be misleading for imbalanced data)
- **Precision**: How many predicted positives are actually positive. Help reduce false positives
- **Recall**: How many actual positives are correctly identified`. Help reduce false negatives
- **F1 Score**: Balance between precision and recall
- **AUC**: Quality of binary classification
- **MAE**: Error in regression tasks

> ⚠️ Complex models (CNNs, Transformers) offer high performance but require more compute

### Scalability
- Tree-based models (Random Forest, XGBoost) scale well with large data
- Deep learning models need more resources (GPUs, distributed computing)

### Interpretability
- Simple models (Linear/Logistic Regression, Decision Trees) are easy to explain
- Complex models (Deep Learning, Ensembles) are harder to interpret

| Factor          | Favor Simpler Models           | Favor Complex Models              |
|----------------|--------------------------------|----------------------------------|
| Use Case       | Regulatory compliance          | High accuracy needed             |
| Performance    | Good, but limited              | High with large data             |
| Interpretability| High (e.g., Logistic Regression)| Low (e.g., Deep Learning)         |
| Scalability    | Fast on small/medium data      | Optimized for large-scale ML     |
| Examples       | Decision Trees, Linear Models  | CNNs, XGBoost, Transformers      |

---

## 2.1.4 Interpretability in Model Selection

**Interpretability** = How well humans can understand a model’s predictions

**Why it matters:**
- Required in regulated industries (banking, healthcare, insurance)
- Builds trust and accountability

### Tools & Techniques
- **SageMaker Clarify**: Bias detection and explainability reports
- **SHAP**: Feature importance for each prediction. Offers global and local interpretability
- **LIME**: Simple explanations for individual predictions. Works across all model types

| Aspect                  | Description                                               |
|--------------------------|-----------------------------------------------------------|
| Business Transparency    | Helps users understand why a decision was made            |
| Regulatory Compliance    | Meets legal requirements for AI interpretability          |
| SageMaker Clarify        | AWS tool for bias + model explainability                  |
| SHAP                     | Feature importance per prediction                         |
| LIME                     | Simple explanation for individual decisions               |

> **Tip**
> Using Amazon SageMaker Clarify to run bias reports on both the training data and the model’s predictions is the best approach to effectively identify and mitigate bias.
---

## 2.1.5 Cost Considerations

**Cost factors:** Training, inference, infrastructure, storage

### How to Save with SageMaker
- Use **Spot Instances** for up to 90% savings (ideal for hyperparameter tuning jobs)
- Use **JumpStart** for pre-trained models (no need to train from scratch and **no-code/low-code**)
- Use **on-demand endpoints** (pay only when in use)

| Strategy                    | Cost Benefit                                 |
|-----------------------------|----------------------------------------------|
| Spot Instances              | Up to 90% savings on training compute        |
| Pre-trained Models (JumpStart) | Skip expensive model training               |
| On-Demand Endpoints         | Pay-per-use (billed per second)              |
| Fewer Labeling Requirements | No need for manual annotation services       |
| Lightweight Infrastructure  | No EC2 or Kubernetes setup required          |

### Extra: JumpStart

| Benefit | Description |
|--------|-------------|
| ✅ Less Compute Resources | No need to train from scratch; use pre-trained models |
| ✅ No Large Datasets | Skip costly data labeling; fine-tune with smaller datasets |
| ✅ On-Demand Deployment | Use AWS-managed endpoints and pay only when in use |

#### Methods available in Jumpstart

- Fine-tuning a pre-trained model with their own dataset enables the team to improve the model’s performance for their specific task, making it more suitable for their use case.

- Prompt engineering allows the team to fine-tune the inputs provided to the model, guiding it to produce more relevant responses for their specific use case. 

---

## 2.1.6 Model Ensembling

Combining predictions from multiple models to improve accuracy and robustness.

- **Bagging**: Trains many weak models independently and averages their predictions (e.g., Random Forest)
- **Boosting**: Trains models sequentially, each correcting errors from the last (e.g., XGBoost, AdaBoost)
- **Stacking**: Combines outputs from multiple models using a meta-model for final prediction. It leverages the complementary strengths of each base model

> **Tip:**
> - Use **boosting** to reduce bias and variance
> - Use **stacking** to combine different model types for best results
