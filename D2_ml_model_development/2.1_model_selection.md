
# 2.1 Model Selection: Comparing and Choosing the Right Algorithm

Selecting the best machine learning model depends on:
- **Use case** (What are you trying to predict or classify?)
- **Data characteristics** (Size, type, structure, balance)
- **Business requirements** (Accuracy, interpretability, cost, speed)

No single model is best for all tasks. Each has strengths and trade-offs. Use the guide below to quickly compare common AWS ML algorithms and their best-fit scenarios.


- [2.1.1 Common AWS ML Algorithms](#211-common-aws-ml-algorithms)
  - [Random Cut Forest (RCF)](#random-cut-forest-rcf)
  - [DeepAR](#deepar)
  - [Linear Learner](#linear-learner)
  - [Time Series K-Means](#time-series-k-means)
  - [XGBoost](#xgboost)
  - [K-Nearest Neighbors (k-NN)](#k-nearest-neighbors-k-nn)
  - [Factorization Machines](#factorization-machines)
- [2.1.2 Key Considerations When Selecting a Model](#212-key-considerations-when-selecting-a-model)
- [2.1.3 Algorithm Trade-offs](#213-algorithm-trade-offs)
- [2.1.4 Interpretability in Model Selection](#214-interpretability-in-model-selection)
- [2.1.5 Cost Considerations](#215-cost-considerations)
- [2.1.6 Model Ensembling](#216-model-ensembling)

---

## 2.1.1 Common AWS ML Algorithms


### Random Cut Forest (RCF)
**Best for:** **Anomaly detection** in time series or sensor data
- **Unsupervised:** Finds unusual patterns without labels
- **Great for:** Detecting equipment failure, fraud, or outliers
- **Limitation:** Not ideal for tasks needing labeled data


### DeepAR
**Best for:** **Forecasting future values** in time series
- **Deep learning:** Models complex trends and dependencies
- **Great for:** Predicting sales, demand, or health risks
- **Limitation:** Not designed for anomaly detection


### Linear Learner
**Best for:** **Supervised learning** (classification/regression)
- **Handles class imbalance:** Uses class weights
- **Great for:** Churn prediction, binary classification
- **Simple, fast, and easy to use**
- **Limitation:** May miss complex patterns in data


### Time Series K-Means
**Best for:** **Clustering similar time series patterns**
- **Groups by similarity:** Finds similar time series
- **Limitation:** Clustering alone does not detect anomalies or provide precise predictions


### XGBoost
**Best for:** **Structured/tabular data**, **imbalanced datasets**
- **Powerful gradient boosting algorithm**
- **Great for:** Fraud detection, competitions, optimizing precision/recall
- **Handles class imbalance and overfitting well**
- **Limitation:** Not designed for recommendations or unstructured data


### K-Nearest Neighbors (k-NN)
**Best for:** **Simple classification** based on similarity
- **Easy to understand and implement**
- **Limitation:** Slow on large datasets, struggles with high-dimensional data


### Factorization Machines
**Best for:** **Sparse, high-dimensional data** (e.g., recommendations)
- **Captures feature interactions efficiently**
- **Great for:** Click prediction, collaborative filtering, recommendations

---

## 2.1.2 Key Considerations When Selecting a Model

1. **Understand the Use Case**
   - What problem are you solving? (e.g., classification, regression, anomaly detection)
   - Do you need interpretability or just accuracy?
2. **Data Characteristics**
   - Is your data labeled or unlabeled?
   - Is it balanced or imbalanced?
   - Is it structured (tables) or unstructured (text, images)?
3. **Business Constraints**
   - Do you need fast predictions (real-time)?
   - Are there cost or infrastructure limits?

---

## 2.1.3 Algorithm Trade-offs

### Performance Metrics
- **Accuracy**: % of correct predictions (can be misleading for imbalanced data)
- **Precision**: How many predicted positives are actually positive
- **Recall**: How many actual positives are correctly identified
- **F1 Score**: Balance between precision and recall
- **AUC**: Quality of binary classification
- **MAE**: Error in regression tasks

> ‚ö†Ô∏è Complex models (CNNs, Transformers) offer high performance but require more compute

### Scalability
- Tree-based models (Random Forest, XGBoost) scale well with large data
- Deep learning models need more resources (GPUs, distributed computing)

### Interpretability
- Simple models (Linear/Logistic Regression, Decision Trees) are easy to explain
- Complex models (Deep Learning, Ensembles) are harder to interpret

| Factor          | Favor Simpler Models           | Favor Complex Models              |
|----------------|--------------------------------|----------------------------------|
| Use Case       | Regulatory compliance          | High accuracy needed             |
| Performance    | Good, but limited              | High with large data             |
| Interpretability| High (e.g., Logistic Regression)| Low (e.g., Deep Learning)         |
| Scalability    | Fast on small/medium data      | Optimized for large-scale ML     |
| Examples       | Decision Trees, Linear Models  | CNNs, XGBoost, Transformers      |

---

## 2.1.4 Interpretability in Model Selection

**Interpretability** = How well humans can understand a model‚Äôs predictions

**Why it matters:**
- Required in regulated industries (banking, healthcare, insurance)
- Builds trust and accountability

### Tools & Techniques
- **SageMaker Clarify**: Bias detection and explainability reports
- **SHAP**: Feature importance for each prediction
- **LIME**: Simple explanations for individual predictions

| Aspect                  | Description                                               |
|--------------------------|-----------------------------------------------------------|
| Business Transparency    | Helps users understand why a decision was made            |
| Regulatory Compliance    | Meets legal requirements for AI interpretability          |
| SageMaker Clarify        | AWS tool for bias + model explainability                  |
| SHAP                     | Feature importance per prediction                         |
| LIME                     | Simple explanation for individual decisions               |

---

## 2.1.5 Cost Considerations

**Cost factors:** Training, inference, infrastructure, storage

### How to Save with SageMaker
- Use **Spot Instances** for up to 90% savings (good for jobs that can be interrupted)
- Use **JumpStart** for pre-trained models (no need to train from scratch)
- Use **on-demand endpoints** (pay only when in use)

| Strategy                    | Cost Benefit                                 |
|-----------------------------|----------------------------------------------|
| Spot Instances              | Up to 90% savings on training compute        |
| Pre-trained Models (JumpStart) | Skip expensive model training               |
| On-Demand Endpoints         | Pay-per-use (billed per second)              |
| Fewer Labeling Requirements | No need for manual annotation services       |
| Lightweight Infrastructure  | No EC2 or Kubernetes setup required          |

---

## 2.1.6 Model Ensembling

Combining predictions from multiple models to improve accuracy and robustness.

- **Bagging**: Trains many weak models independently and averages their predictions (e.g., Random Forest)
- **Boosting**: Trains models sequentially, each correcting errors from the last (e.g., XGBoost, AdaBoost)
- **Stacking**: Combines outputs from multiple models using a meta-model for final prediction

> **Tip:**
> - Use **boosting** to reduce bias and variance
> - Use **stacking** to combine different model types for best results

It may not be as effective in capturing complex patterns in the data as more sophisticated algorithms like XGBoost.

 The Linear Learner algorithm in Amazon SageMaker includes a built-in 'class_weights' parameter that allows you to assign different weights to classes, effectively handling imbalanced datasets by emphasizing underrepresented classes during training.
### Time Series K-Means Algorithm

Time Series K-Means can cluster similar time series patterns, but clustering alone does not provide the precision needed for real-time anomaly detection, which is crucial for predictive maintenance.

### XGBoost algorithm

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:

Its robust handling of a variety of data types, relationships, distributions.

The variety of hyperparameters that you can fine-tune.

XGBoost is a powerful gradient boosting algorithm that excels in structured data problems, such as fraud detection. It allows for custom objective functions, making it highly suitable for optimizing precision and recall, which **are critical in imbalanced datasets**. Additionally, XGBoost has built-in techniques for handling class imbalance, such as scale_pos_weight.

While XGBoost is a powerful algorithm for general-purpose machine learning tasks, it is not specifically designed for recommendation systems.

XGBoost and overfitting: Reducing the max_depth parameter effectively limits the complexity of the decision trees, preventing the model from fitting noise and thus improving its ability to generalize to unseen data.

### K-Nearest Neighbors
K-Nearest Neighbors (k-NN) can classify based on similarity, but it does not scale well with large datasets and may struggle with the high-dimensional, imbalanced nature of the data in this context.

### Factorization Machines algorithm

The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. **Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation**.

Factorization Machines is well-suited for collaborative filtering. It excels at modeling sparse user-item interactions, making it ideal for large-scale recommendation systems where there are many users and items but relatively few interactions for each user-item pair. This algorithm can effectively capture latent factors to provide personalized recommendations.

## Key Considerations When Selecting a Model

### 1. **Use Case Understanding**
- Always start by identifying what you want to solve.
- Choose simpler models like **Logistic Regression** when interpretability is important.
- Choose more complex models like **Neural Networks** when you need high accuracy on non-linear data.

## Algorithm Trade-offs

### **Performance Metrics**
- **Accuracy**: % of correct predictions. It can be misleading in imbalanced datasets.
- **Precision**: TP / (TP + FP). It measures how many predicted positives are actually positive, helping reduce false positives
- **Recall**: TP / (TP + FN). It measures how many actual positives are correctly identified, helping reduce false negatives.
- **F1 Score**: Harmonic mean of Precision & Recall  
- **AUC (Area Under the Curve)**: Binary classification quality  
- **MAE (Mean Absolute Error)**: Regression error measure  

‚ö†Ô∏è Complex models like **CNNs** or **Transformers** offer better performance but **need more computational power**.

### **Scalability**
- Related to **training and inference speed**
- **Tree-based models** (e.g., Random Forest, XGBoost):
  - Scale well with large datasets
  - Often require GPUs or distributed computing

### **Interpretability**
- **Simple models** like:
  - Linear Regression
  - Logistic Regression (Logistic regression is a statistical method designed for binary classification problems)
  - Decision Trees  
  Are **easier to explain** and **regulatory compliant**

## Summary Table

| Factor          | Favor Simpler Models           | Favor Complex Models              |
|----------------|--------------------------------|----------------------------------|
| Use Case       | Regulatory compliance          | High accuracy needed             |
| Performance    | Good, but limited              | High with large data             |
| Interpretability| High (e.g., Logistic Regression)| Low (e.g., Deep Learning)         |
| Scalability    | Fast on small/medium data      | Optimized for large-scale ML     |
| Examples       | Decision Trees, Linear Models  | CNNs, XGBoost, Transformers      |

# Considerations for Interpretability in Model Selection

## What is Interpretability?

**Interpretability** is the ability of humans to **understand the reasoning** behind a machine learning model‚Äôs prediction or output.

- Important in high-stakes domains like **banking**, **insurance**, and **medicine**
- Builds **trust** with users
- Helps meet **compliance** requirements from government and regulatory agencies

## Importance of Interpretability

### üîç Business Transparency
- Clients and stakeholders must understand model decisions.
- Promotes **accountability** and **trust** in ML systems.

### Regulatory Compliance
- In sectors like **insurance**, **healthcare**, and **finance**, regulations demand clear decision logic.
- Regulatory bodies (e.g., **BSP** in the Philippines) enforce **fair and explainable** AI practices.

## Tools & Techniques for Model Explainability

### SageMaker Clarify
- Detects **bias** and delivers **explainability reports**
- Works **before and after model training**
- Seamlessly integrates with SageMaker pipelines

Using Amazon SageMaker Clarify to run bias reports on both the training data and the model‚Äôs predictions is the best approach to effectively identify and mitigate bias. This method provides a comprehensive analysis of fairness by measuring pre-training bias in the dataset and post-training bias in the model outputs.

### SHAP (SHapley Additive exPlanations)
- Assigns importance to each feature in a prediction
- Offers **global and local interpretability**

### LIME (Local Interpretable Model-Agnostic Explanations)
- Provides simple explanations for **individual predictions**
- Works across all model types

## Summary Table

| Aspect                  | Description                                               |
|--------------------------|-----------------------------------------------------------|
| Business Transparency    | Helps users understand why a decision was made            |
| Regulatory Compliance    | Meets legal requirements for AI interpretability          |
| SageMaker Clarify        | AWS tool for bias + model explainability                  |
| SHAP                     | Feature importance per prediction                         |
| LIME                     | Simple explanation for individual decisions               |

# Cost Considerations in Model Selection

When building ML solutions, **cost** is a major factor. This includes training, inference, infrastructure, and storage.

## Cost Evaluation with AWS SageMaker

### Training Cost Optimization
- **SageMaker charges per compute and storage usage**
- Use **Spot Instances** to reduce training costs by **up to 90%**
- Spot instances are ideal for hyperparameter tuning jobs because they offer significant cost savings and are suitable for workloads that can tolerate interruptions.

## SageMaker JumpStart for Cost-Effective ML

### What is JumpStart?
- A **no-code/low-code** solution with **pre-trained models**
- Eliminates the need for extensive training from scratch

### How JumpStart Helps Reduce Costs

| Benefit | Description |
|--------|-------------|
| ‚úÖ Less Compute Resources | No need to train from scratch; use pre-trained models |
| ‚úÖ No Large Datasets | Skip costly data labeling; fine-tune with smaller datasets |
| ‚úÖ On-Demand Deployment | Use AWS-managed endpoints and pay only when in use |

### Methods available in Jumpstart

- Fine-tuning a pre-trained model with their own dataset enables the team to improve the model‚Äôs performance for their specific task, making it more suitable for their use case.

- Prompt engineering allows the team to fine-tune the inputs provided to the model, guiding it to produce more relevant responses for their specific use case. 

## Summary Table

| Strategy                    | Cost Benefit                                 |
|-----------------------------|----------------------------------------------|
| Spot Instances              | Up to 90% savings on training compute        |
| Pre-trained Models (JumpStart) | Skip expensive model training               |
| On-Demand Endpoints         | Pay-per-use (billed per second)              |
| Fewer Labeling Requirements | No need for manual annotation services       |
| Lightweight Infrastructure  | No EC2 or Kubernetes setup required          |

# Model Ensembling

## Model Ensembling Techniques
Combining predictions from multiple models to improve accuracy and robustness.

- **Bagging (Bootstrap Aggregating)**  
  Trains many weak models independently and averages their predictions.  
  *Example:* Random Forest.

- **Boosting**  
  Trains models sequentially where each new model focuses on correcting errors from previous ones.  
  *Example:* XGBoost, AdaBoost.

  Boosting is the best-suited ensemble method for reducing both bias and variance while enabling flexible modeling of complex data behaviors. 

- **Stacking**  
  Combines outputs from multiple models and inputs them into a final meta-model for prediction.

  Stacking is the most effective method to combine logistic regression, decision tree, and support vector machine models because it leverages the complementary strengths of each base model by training a meta-model on their predictions. This approach improves overall accuracy and robustness beyond what simple voting, bagging, or boosting (especially with heterogeneous models) can achieve.
