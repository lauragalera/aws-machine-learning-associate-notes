
# 2.2 Model Training, Tuning, and Optimization

- [2.2.1 Hyperparameter Tuning](#221-hyperparameter-tuning)
  - [Common Tuning Methods](#common-tuning-methods)
  - [SageMaker Automatic Model Tuning (AMT)](#sagemaker-automatic-model-tuning-amt)
  - [Summary Table](#summary-table)
  - [Tips for Tuning](#tips-for-tuning)
- [2.2.2 Model Hyperparameters and Performance](#222-model-hyperparameters-and-performance)
  - [What Are Hyperparameters?](#what-are-hyperparameters)
  - [Important Hyperparameters Examples](#important-hyperparameters-examples)
  - [Balancing Bias and Variance](#balancing-bias-and-variance)
  - [Key Takeaways](#key-takeaways)
- [2.2.3 Methods to Reduce Model Training Time](#223-methods-to-reduce-model-training-time)
  - [Early Stopping](#early-stopping)
  - [Distributed Training](#distributed-training)
  - [Model Size Reduction](#model-size-reduction)
  - [Summary Table](#summary-table-1)
- [2.2.4 Factors Influencing Model Size](#224-factors-influencing-model-size)
  - [Model Architecture & Data Impact](#model-architecture--data-impact)
  - [Trade-offs: Complexity vs. Resources](#trade-offs-complexity-vs-resources)
  - [AWS Best Practices for Model Size Optimization](#aws-best-practices-for-model-size-optimization)
  - [Key Takeaways](#key-takeaways-1)
- [2.2.5 Improving Model Performance](#225-improving-model-performance)
  - [Feature Engineering](#feature-engineering)
  - [Hyperparameter Tuning](#hyperparameter-tuning)
  - [Regularization Techniques: Preventing Overfitting](#regularization-techniques-preventing-overfitting)
  - [Tips for Performance](#tips-for-performance)
- [2.2.6 Managing Catastrophic Forgetting in Neural Networks](#226-managing-catastrophic-forgetting-in-neural-networks)
  - [Solutions](#solutions)

---

## 2.2.1 Hyperparameter Tuning

Hyperparameter tuning is the process of finding the **best combination of hyperparameters** to maximize the performance of an ML model.

### Common Tuning Methods

| Method              | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **Random Search**   | Fast and trial-and-error based. Picks random hyperparameter combinations.   |
| **Grid Search**     | Tries every possible combination. More accurate but slower.                 |
| **Bayesian Optimization** | Uses probability models to estimate best parameters. Accurate but computationally intensive. |
| **Hyperband**       | Stops underperforming configurations early, focusing resources on the best. |

### SageMaker Automatic Model Tuning (AMT)

SageMaker AMT automates hyperparameter tuning using **Bayesian Optimization**.

**Key Features:**
- **Parallel training:** Tests multiple configurations simultaneously
- **Cost-efficient:** Minimizes compute waste
- **Automated:** No need for manual training loops

### Summary Table

| Method        | Speed       | Accuracy     | Best Use Case                        |
|---------------|-------------|--------------|--------------------------------------|
| Random Search | Fast        | Moderate     | Quick experimentation                |
| Grid Search   | Slow        | High         | Small parameter spaces               |
| Bayesian Opt. | Moderate    | Very High    | Complex models, performance-focused  |
| Hyperband     | Very Fast   | High         | Large search spaces, limited time    |

### Tips for Tuning
- Use **SageMaker AMT** to automate and speed up hyperparameter tuning.
- Combine **Hyperband** with AMT for scalable, efficient optimization.
- Explore **Bayesian Optimization** for critical applications like NLP, recommendation, and fraud detection.

---

## 2.2.2 Model Hyperparameters and Performance

### What Are Hyperparameters?

Hyperparameters are configuration settings that control how your machine learning model learns. They are set before training and are crucial to performance tuning.

> **Note:** Hyperparameters are different from parameters. Parameters are learned by the model during training.

### Important Hyperparameters Examples

| Hyperparameter       | Model Type                | Impact                                    |
|---------------------|---------------------------|-------------------------------------------|
| **Tree Depth**       | Decision Trees, Random Forests | Controls complexity; deeper trees can overfit |
| **Learning Rate**    | Most ML models            | Speed of weight updates; too high = unstable, too low = slow training |
| **Number of Layers** | Deep Learning Networks    | More layers = more powerful but computationally expensive |

> **Tip:** 
>Increasing batch size can speed up training, but reduce epochs to avoid overfitting. Always monitor learning rate.
>
> If your training and validation loss keep bouncing, it is usually due to an excessively high **learning rate**. Try lowering the learning rate to allow the model to converge more smoothly.

### Balancing Bias and Variance

| Concept | Description | Effect on Model |
|---------|-------------|-----------------|
| **Bias** | Model too simple; can't capture data patterns | Underfitting; poor accuracy on both train and test data |
| **Variance** | Model too complex; fits training data too well | Overfitting; good train accuracy but poor generalization |

**Goal:** Find the optimal trade-off to avoid underfitting and overfitting.

> **Rememeber:** A model with both low bias and low variance is ideal and should perform well on new data.

### Key Takeaways
- Hyperparameter tuning helps optimize model accuracy and efficiency.
- Adjust parameters carefully to balance bias and variance.
- Use domain knowledge and validation data to guide tuning decisions.

---

## 2.2.3 Methods to Reduce Model Training Time

Training a model is like preparing for a big event â€” the right strategies reduce wasted time and effort.

### Early Stopping
- **What it is:** Automatically halts training when model performance stops improving.
- **Why it matters:** Prevents overfitting and saves compute time.
- **AWS Implementation:** Use **Amazon SageMaker Automatic Model Tuning** to monitor metrics and stop training when optimal performance is reached.

### Distributed Training
- **What it is:** Splits training workload across multiple compute instances.
- **Why it matters:** Greatly accelerates training, especially for large datasets or deep learning models.
- **AWS Implementation:**
  - **Data Parallelism:** Split data across workers.
  - **Model Parallelism:** Split the model across GPUs/instances.

### Model Size Reduction
- **Pruning:** Remove unnecessary neurons or layers.
- **Quantization:** Reduce the precision of model weights (e.g., from 32-bit to 8-bit).

---

## 2.2.4 Factors Influencing Model Size

The size of a machine learning model affects performance, cost, and speed. A good model balances **accuracy**, **efficiency**, and **resource usage**.

### Model Architecture & Data Impact
- **Model architecture** and **input data** volume influence the final model size.
- **More parameters = larger model = slower inference**.

**AWS Tool:** Use **Amazon SageMaker Neo** to compile and optimize models for faster and more scalable inference across platforms.

### Trade-offs: Complexity vs. Resources

| Model Type              | Pros                                | Cons                                 |
|-------------------------|--------------------------------------|--------------------------------------|
| Simple (e.g., linear)   | Fast and low-cost                    | Less accurate                        |
| Complex (e.g., deep learning) | High accuracy                      | Slower, needs more compute & storage |

### AWS Best Practices for Model Size Optimization
- **Pruning:** Remove unnecessary parts of the model.
- **Quantization:** Reduce the precision of model weights.
- **SageMaker Model Optimizer:** Automatically reduce model complexity while keeping performance high.

### Key Takeaways
- Balance **accuracy** and **efficiency** in your model design.
- Use AWS tools like:
  - **SageMaker Neo** for faster, optimized deployment.
  - **SageMaker Model Optimizer** for simplifying models.
- Efficient models = lower costs + better scalability.

---

## 2.2.5 Improving Model Performance

Model performance refers to how well an ML model **generalizes to new, unseen data**. Improving it requires careful data handling, model configuration, and overfitting prevention.

### Feature Engineering
- Transform raw data into meaningful inputs for your model.
- Adds domain-specific insight that boosts accuracy.

### Hyperparameter Tuning
- Fine-tune parameters that control the learning process (e.g., learning rate, number of layers).
- Can be automated in AWS using **SageMaker Automatic Model Tuning**.

### Regularization Techniques: Preventing Overfitting

Overfitting = Model does well on training data but fails on unseen data.

#### Common Regularization Methods

| Technique       | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| **L1 (Lasso)**  | Reduces some feature weights to zero, simplifying the model.                |
| **L2 (Ridge)**  | Keeps all weights but reduces large values, balancing influence.            |
| **Dropout**     | In deep learning, randomly disables neurons during training for robustness. |

### Tips for Performance
- Use **SageMaker Automatic Model Tuning** to search for the best hyperparameters.
- Regularize your models using **L1**, **L2**, or **Dropout** (for deep learning).
- Apply **feature engineering** to extract behavior or context from your raw data.
---

## 2.2.6 Managing Catastrophic Forgetting in Neural Networks

**Catastrophic forgetting** occurs when a neural network loses previously learned knowledge after training on new data.

### Solutions
- **Replay Memory:** Mixes old and new data during training to preserve past knowledge. Incremental learning and transfer learning can help.
- **Elastic Weight Consolidation (EWC):** Retains important weights from previous training to maintain prior learning.
- **Progressive Networks:** Adds new layers for new tasks while keeping old layers intact to preserve earlier knowledge.



