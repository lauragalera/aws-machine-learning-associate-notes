# Understanding Pre-Training Bias Metrics

## Bias Matters
Bias in machine learning data can lead to **unfair**, **discriminatory**, or **ineffective** models. Identifying bias **before training** helps improve fairness and accuracy.

Conditional Demographic Disparity (CDD) measures the difference in positive prediction rates between demographic groups, while conditioning on relevant features like income. This allows you to identify subtle biases that might be masked when looking only at overall predictions, ensuring that the model's decisions are fair across different groups given their specific circumstances.

## Class Imbalance (CI)

Class imbalance occurs when **one class heavily outnumbers another**. This skews model predictions.

### Example:
In a Spanish e-commerce fraud detection dataset:
- 98% of transactions are legitimate
- 2% are fraudulent

➡️ The model might always predict "legitimate" and miss fraudulent activity.

## Difference in Proportions of Labels (DPL)

DPL measures how **labels are distributed across subgroups** (e.g., region, gender, income). It highlights potential **group bias**.

## Bias in Different Data Types

| Data Type     | Example of Potential Bias                                   |
|---------------|-------------------------------------------------------------|
| **Numeric**    | Income level → More approvals for higher earners            |
| **Text**       | Reviews may reflect bias against certain cultural groups    |
| **Image**      | Facial recognition favors certain features or skin tones    |

## Using Amazon SageMaker Clarify

**SageMaker Clarify** provides tools for:
- **Bias detection** before training (e.g., CI, DPL)
- **Fairness monitoring** during and after training

### What It Can Do:
- Analyze feature distributions and label proportions
- Visualize bias metrics
- Help ML teams make **fairer data and model choices**

Feature engineering is one of the most effective ways to boost model performance, particularly in domain-specific applications like credit risk modeling. By creating more informative features, you can provide the model with better signals for prediction. SageMaker Clarify can be used to evaluate feature importance, helping you identify the most impactful features and further refine the model.

The F1 score balances precision and recall, making it suitable for imbalanced datasets. AUC-ROC provides a comprehensive view of model performance across different thresholds. You must also assess whether the model gives different demographic groups similar true positive rates, making it a key measure for detecting bias and ensuring fairness in sensitive applications like loan approvals.

## Summary Table

| Metric                        | Purpose                              | Example Use Case                         |
|------------------------------|--------------------------------------|------------------------------------------|
| Class Imbalance (CI)         | Detect dominant vs minority classes  | Fraud detection, rare disease prediction |
| Difference in Proportions    | Detect subgroup label disparity      | Regional loan approval rates             |
| SageMaker Clarify            | Tool to analyze and monitor bias     | Identify label skew and feature bias     |

# Identifying and Mitigating Bias in Data

## Amazon SageMaker Clarify

A powerful AWS tool to:
- Detect bias **before, during, and after** training
- Provide **bias reports**
- Offer **model explainability** insights

## Bias Types and How to Mitigate Them

| Bias Type         | Description | Mitigation Approach |
|-------------------|-------------|----------------------|
| **Selection Bias** | Data not representing the entire population | Data balancing (e.g., oversampling, stratified sampling) |
| **Measurement Bias** | Inaccurate or faulty data collection | Improve instruments, standardize data collection |
| **Label Bias**     | Subjective or inconsistent labeling | Use multiple reviewers, consensus labeling, or QA processes |

## Model Explainability with Clarify

SageMaker Clarify provides insights into:
- **Feature importance** (which inputs affect decisions)
- **Transparency & accountability** for regulated use cases (e.g., finance, healthcare)

### When to Use:
- During **data preparation**
- For **auditing**, **regulatory reviews**, or **ethical assessments**

## Summary

- Always **evaluate for bias** at each ML pipeline stage.
- Use **SageMaker Clarify** to detect and resolve biases.
- Understand **why your model behaves the way it does** using explainability features.

# Preparing Data for Model Training

## Core Techniques for Data Preparation

### **Dataset Splitting**
Dividing data into:
- **Training set** (70–80%)
- **Validation set** (10–20%)
- **Test set** (10%)

### **Shuffling**
Randomizes dataset to remove unwanted ordering or biases.

### **Data Augmentation**
Generates synthetic data to enhance model generalization.

- Especially useful in **computer vision** (e.g., flips, rotations).

### **Feature Scaling and Normalization**
Ensures features are on a consistent scale.

- Use tools like **MinMaxScaler** or **RobustScaler** in **SageMaker Data Wrangler**.

## Efficient Storage for Training Data

### **Amazon Elastic File System (EFS)**
- Best for **real-time** training across **multiple EC2 instances**.
- Scalable and shared.

### **Amazon FSx for Lustre**
- Designed for **HPC workloads**.
- Delivers **sub-millisecond latency**.

## Summary

Data preparation is critical for model accuracy and fairness:
- Always **shuffle and split datasets** properly.
- Use **data augmentation** to improve model generalization.
- Apply **scaling techniques** for consistency.
- Store data in **scalable, low-latency systems** like EFS and FSx based on your use case.
