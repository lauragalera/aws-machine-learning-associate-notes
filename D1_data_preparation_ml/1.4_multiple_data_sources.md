# 1.4 Multi-Source Data Integration

- [1.4.1 Processing Techniques by Scale](#141-processing-techniques-by-scale)
  - [Python (Pandas)](#python-pandas)
  - [PySpark](#pyspark)
- [1.4.2 AWS Glue for Multi-Source Integration](#142-aws-glue-for-multi-source-integration)
  - [Core Components](#core-components)
  - [Example: E-commerce Data Pipeline](#example-e-commerce-data-pipeline)

---

## 1.4.1 Processing Techniques by Scale

###  Python (Pandas)
- **Best For**: Small to medium-sized datasets; in-memory operations
- **Usage Scenario**: Merging CSV files (sales transactions with customer demographics)
- **Capabilities**: Inner, left, right, and outer joins via `merge()`, grouping and aggregation, fast prototyping

### PySpark
- **Best For**: Large-scale, distributed datasets stored in the cloud
- **Usage Scenario**: Merging S3-stored logs with relational data from RDS or Redshift
- **Capabilities**: Distributed joins, petabyte-scale processing, lazy evaluation optimization

---

## 1.4.2 AWS Glue for Multi-Source Integration

### Core Components
| Component        | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Crawlers**     | Automatically discover and classify schema from data sources               |
| **Data Catalog** | Centralized metadata repository used across Glue, Athena, Redshift Spectrum |
| **Glue Jobs**    | Spark-based ETL scripts (Python or Scala) for transformations             |
| **Workflows**    | Define multi-step ETL pipelines using visual DAGs                         |
| **Triggers**     | Schedule or event-based mechanisms for automatic job execution             |
| **Monitoring**   | Track performance, logs, errors through CloudWatch and built-in UI        |

### Example: E-commerce Data Pipeline
**Data Sources**:
- **S3**: Order transactions
- **Amazon RDS**: Customer profiles  
- **DynamoDB**: Real-time inventory changes

**Glue Implementation**:
1. **Schema Discovery**: Crawlers generate schema for all sources
2. **ETL Processing**: Jobs join datasets on customer ID and order ID
3. **Output**: Transformed, merged datasets to S3 or Redshift for ML workflows