# AWS Data Sources & Storage for ML

## Data Sources for Ingestion

### Databases
- Services: `RDS`, `Aurora`, `DynamoDB`, `Redshift`
- Access: JDBC/ODBC connectors
- Use **AWS DMS** for migrations or CDC

### Data Lakes (Amazon S3)
- Transfer tools: `AWS DataSync`, `S3 Transfer Acceleration`
- Massive ingestion: `Snowball`, `Snowmobile`

### Streaming Data
- **Amazon Kinesis** family:
  - `Data Streams`: Real-time ingestion
  - `Firehose`: Buffering and automatic S3/Redshift loading. Configuring Kinesis Data Firehose to buffer data based on time intervals helps in batch processing, which reduces API calls and helps maintain data consistency.
  
  Firehose is much used with lambda functions; set AWS Lambda as a data transformation function within Amazon Kinesis Data Firehose and enable source record backup to Amazon S3.

### File Systems / Warehouses
- `FSx`, `EFS`: Low-latency or shared ML workloads
- `Redshift`: Scalable data warehouse for analytical workloads

## AWS Storage Services

### Amazon S3 – Core ML Data Lake

**Storage Classes**

| Class                   | Retrieval Time     | Use Case                              |
|------------------------|--------------------|----------------------------------------|
| Standard               | Milliseconds       | Frequently accessed ML datasets        |
| Intelligent-Tiering    | Milliseconds       | Unpredictable access patterns          |
| Standard-IA            | Milliseconds       | Archived models, old validation sets   |
| Glacier                | Minutes to hours   | Long-term archiving                    |
| Glacier Deep Archive   | Up to 12 hours     | Compliance or rarely retrieved data    |

**Encryption Options**
- `SSE-S3`: S3-managed keys
- `SSE-KMS`: KMS-managed (auditable)
- `SSE-C`: Customer-provided keys

**Performance Tips**
- Use prefix-based partitioning for faster lookups
- Leverage multipart uploads and `S3 Transfer Acceleration` for large files

**Ingestion from S3**

Pipe input mode is designed for large datasets, allowing data to be streamed directly from Amazon S3 into the training instances. This minimizes disk usage and allows training to begin immediately as the data streams in, making it ideal for your scenario where high throughput and efficiency are critical.

In pipe mode, data is pre-fetched from Amazon S3 at high concurrency and throughput, and streamed into a named pipe, which also known as a First-In-First-Out (FIFO) pipe for its behavior. Each pipe may only be read by a single process.

### Amazon EFS – Shared File Access

**Modes**
- `General Purpose`: Low latency (e.g. shared notebooks)
- `Max I/O`: High throughput (e.g. parallel training)

**Optimizations**
- `EFS-IA`: Lower-cost option for less-frequent access
- Lifecycle rules to auto-transition cold data

### Amazon FSx for Lustre – High-Performance Training Storage

**Deployment Types**
- `Scratch`: Temporary high-speed storage
- `Persistent`: Durable across training jobs

**Features**
- Compression, deduplication, snapshotting
- Integrates with S3 as a fast-access cache

### AWS Lake Formation

AWS Lake Formation is specifically designed for aggregating and managing large datasets from various sources, including Amazon S3, databases, and other on-premises or cloud-based sources. It simplifies the process of:

Integrating data from various locations.

Cataloging the data in a centralized data lake.

Managing permissions and security for the aggregated dataset.

AWS Lake Formation supports connecting to on-premises PostgreSQL databases and Amazon S3, making it the best choice for aggregating transaction logs, customer profiles, and database tables. Additionally, the centralized data lake can be used for further analysis and ML training.

AWS Lake Formation provides a robust and scalable solution for enforcing fine-grained access control in data lakes by leveraging tag-based permissions. By assigning tags to datasets and defining permissions based on these tags, it allows data scientists to access only the data relevant to their assigned product categories. This method integrates seamlessly with Amazon Athena, enabling secure and efficient querying without the need for complex bucket or folder-level policies.

## Summary

Choosing the right AWS storage service is critical for balancing performance, cost, scalability, and workload requirements. Consider these factors:

- **Data Access Patterns:** Frequent access (e.g., training datasets) vs. infrequent access (archived models).
- **Throughput Requirements:** High throughput for parallel training or real-time inference workloads.
- **Scalability:** Ability to grow with expanding data volumes.
- **Cost:** Optimize for budget while meeting performance needs.

| Service                | Type    | Performance                      | Scalability | Cost Model        | Use Cases                                           |
|------------------------|---------|---------------------------------|-------------|-------------------|----------------------------------------------------|
| **Amazon S3**          | Object  | High throughput, scalable        | Massive     | Tiered, pay-as-you-go | Diverse data types, archiving, integration with many AWS services |
| **Amazon EFS**         | File    | Low latency, scalable throughput | Scalable    | Pay-as-you-go     | Shared access, model training & serving, persistent file storage |
| **Amazon FSx for Lustre** | File | Highest throughput, sub-ms latency | High throughput | Pay-as-you-go  | HPC workloads, deep learning, parallel access for large datasets |
| **Amazon FSx for ONTAP** | File   | High performance, availability, deduplication | High        | Pay-as-you-go     | Hybrid cloud ML, deduplication, data sharing with on-premises systems |