# AWS Services for Data Ingestion

## Amazon Kendra – Semantic Search

**Use Cases**
- Indexing of unstructured content: FAQs, support docs, wikis
- Semantic retrieval of context-aware documents

**ML Integration**
- Use as a source for NLP training or fine-tuning
- Enrich models with high-confidence answers

**Supported Sources**
- `S3`, `Confluence`, `SharePoint`, RDBs, custom connectors

**Customization**
- Extend vocabulary, manage synonyms, adjust ranking
- Evaluate performance with CloudWatch metrics

## AWS Glue – Scalable ETL and Profiling

**Job Types**
- `Spark`: Distributed processing for large-scale ETL
- `Python Shell`: Lightweight scripting jobs

**Triggers**
- Event-driven (e.g., via S3 or EventBridge)
- Scheduled or on-demand

**Features**
- Job bookmarks for **incremental loads**
- Push-down predicates for query efficiency
- Built-in data quality checks and profiling
- Visual transformation with **Glue DataBrew**

## AWS Glue DataBrew – Visual ETL

**Capabilities**
- Drag-and-drop transformation and enrichment
- Build transformation **recipes**

**Functions**
- Handle missing values, outliers, type conversions
- Text processing: tokenization, stemming, etc.

**Profiling**
- Data distributions, value ranges, schema detection

**Integration**
- Export to `SageMaker` in supported ML formats (`CSV`, `Parquet`, `JSON`)

## SageMaker Data Wrangler – End-to-End Data Prep for ML

**Exploration Tools**
- Histograms, scatter plots, correlation matrices

**Feature Engineering**
- Timestamp decomposition
- Categorical encoding, feature interactions

**Workflow Integration**
- Export data prep steps to `SageMaker Pipelines`

## SageMaker Processing Jobs – Custom Batch Pipelines

**Use Cases**
- Data preprocessing (e.g., image resize, cleaning)
- Postprocessing and evaluation
- Batch transformation or format conversion

**Features**
- Bring custom Docker containers (via ECR)
- Access data from S3, output back to S3
- Choose compute instance type (CPU/GPU)

**Integration**
- Works seamlessly within `SageMaker Pipelines`
- Ideal for **non-real-time, large-scale** workloads


## Key Considerations for Data Ingestion Pipelines

- **Data Volume & Velocity**  
  Choose streaming services (Kinesis) for high-throughput, low-latency ingestion.  
  Use batch services (Glue, Batch) when latency is less critical and data volumes are large.

- **Data Transformation Needs**  
  Perform cleaning, enrichment, and feature engineering during ingestion to improve downstream model quality.  
  Visual tools (Data Wrangler) and code-based ETL (Glue, Processing Jobs) provide flexibility.

- **Fault Tolerance & Monitoring**  
  Implement retry policies, error handling, and incremental loading to ensure data consistency.  
  Use CloudWatch and AWS Glue job metrics to monitor pipeline health.

- **Scalability & Cost Optimization**  
  Automatically scale ingestion pipelines based on traffic patterns.  
  Optimize buffering and batching configurations to balance latency and cost.

- **Seamless Integration**  
  Leverage tight integration with storage (S3), compute (SageMaker), and analytics (Athena, Redshift) for end-to-end ML workflows.

# Merging and Processing Data from Multiple Sources

Merging and processing data from disparate sources is a cornerstone of machine learning dataset preparation. Combining structured and semi-structured datasets enhances feature richness, improves representation, and results in more accurate and generalizable ML models.

## Techniques for Data Merging

###  Python (Pandas)
- **Best For:** Small to medium-sized datasets; in-memory operations.
- **Usage Scenario:** Merging CSV files (e.g., sales transactions with customer demographics).
- **Capabilities:**
  - Joins: Inner, left, right, and outer joins via `merge()`.
  - Grouping and aggregation (`groupby`, `pivot_table`).
  - Fast prototyping and debugging.

### PySpark
- **Best For:** Large-scale, distributed datasets stored in the cloud.
- **Usage Scenario:** Merging S3-stored logs with relational data from Amazon RDS or Redshift.
- **Capabilities:**
  - Distributed joins and transformations.
  - Handles petabyte-scale datasets with resilience and fault tolerance.
  - Lazy evaluation for optimization.

## AWS Glue

AWS Glue is a fully managed ETL (Extract, Transform, Load) service that simplifies and automates merging, transformation, and movement of data across AWS services.

### Core Components

| Component        | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Crawlers**     | Automatically discover and classify schema from data sources.               |
| **Data Catalog** | Centralized metadata repository used across Glue, Athena, Redshift Spectrum. |
| **Glue Jobs**    | Spark-based ETL scripts (Python or Scala) that perform transformations.     |
| **Workflows**    | Define multi-step ETL pipelines using visual DAGs.                          |
| **Triggers**     | Schedule or event-based mechanisms to launch jobs automatically.            |
| **Monitoring**   | Track job performance, logs, errors through CloudWatch and built-in UI.     |

### Example Use Case: E-commerce Data Integration
A company wants to unify multiple data sources for ML training:
- **S3:** Order transactions
- **Amazon RDS:** Customer profiles
- **DynamoDB:** Real-time inventory changes

Using Glue, the company:
- Crawls all sources to generate schema.
- Performs ETL jobs to join datasets on customer ID and order ID.
- Outputs transformed, merged datasets to Amazon S3 or Redshift for downstream ML workflows.

### Key Features
- **In-Memory Processing:** Reduces I/O operations, accelerating iterative ML workflows.
- **Scalability:** Handles everything from a laptop to multi-node EMR clusters or Glue backends.
- **Fault Tolerance:** Resilient Distributed Dataset (RDD) model ensures task re-execution on failure.
- **Integration:** Works seamlessly with AWS services like S3, Redshift, RDS, and DynamoDB.