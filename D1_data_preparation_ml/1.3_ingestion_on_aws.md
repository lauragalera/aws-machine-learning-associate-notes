# AWS Services for Data Ingestion

## Amazon Kendra – Semantic Search

Amazon Kendra is the best choice for implementing semantic search on archived articles stored in Amazon S3 because it is specifically designed to provide intelligent, natural language search capabilities. It natively integrates with S3 to index documents and supports semantic understanding of queries, enabling users to retrieve relevant content efficiently without requiring complex custom development.

To securely and efficiently synchronize on-premises data with Amazon Kendra, the best approach involves using AWS DataSync and AWS Transfer Family to handle the secure and automated transfer of data into Amazon S3, which Kendra can index.

Amazon CloudWatch can monitor Amazon Kendra by collecting metrics and logs, and it allows you to create alarms that trigger notifications when error thresholds are breached. It integrates well with AWS CloudFormation for automated deployment and management.

**Use Cases**
- Indexing of unstructured content: FAQs, support docs, wikis
- Semantic retrieval of context-aware documents

**ML Integration**
- Use as a source for NLP training or fine-tuning
- Enrich models with high-confidence answers

**Supported Sources**
- `S3`, `Confluence`, `SharePoint`, RDBs, custom connectors

**Customization**
- Extend vocabulary, manage synonyms, adjust ranking
- Evaluate performance with CloudWatch metrics

## AWS Glue – Scalable ETL and Profiling

**Job Types**
- `Spark`: Distributed processing for large-scale ETL
- `Python Shell`: Lightweight scripting jobs

**Triggers**
- Event-driven (e.g., via S3 or EventBridge)
- Scheduled or on-demand

**Features**
- Job bookmarks for **incremental loads**
- Push-down predicates for query efficiency
- Built-in data quality checks and profiling
- Visual transformation with **Glue DataBrew**

AWS Glue’s Dynamic Frames provide a powerful and flexible way to process semi-structured data common in financial transaction datasets, simplifying schema management and transformation logic. Selecting the 'G.1X' worker type balances computational power and cost efficiency for moderate to large batch workloads, avoiding the unnecessary expense of more powerful worker types unless required.

### FindMatches
AWS Glue FindMatches is a built-in feature designed to detect duplicate records in datasets, even when the records are not exact matches. It uses machine learning to find similarities across key attributes, such as customer names, addresses, and emails.

Key Benefits of AWS Glue FindMatches:

- Minimal coding required - The ML-based approach simplifies the deduplication process.

- Flexible matching logic - Automatically identifies fuzzy matches and near-duplicates.

- Scalable and serverless - Works seamlessly with large datasets in Amazon S3.

## AWS Glue DataBrew – Visual ETL

**Capabilities**
- Drag-and-drop transformation and enrichment
- Build transformation **recipes**

**Functions**
- Handle missing values, outliers, type conversions
- Text processing: tokenization, stemming, etc.

**Profiling**
- Data distributions, value ranges, schema detection

**Integration**
- Export to `SageMaker` in supported ML formats (`CSV`, `Parquet`, `JSON`)

AWS Glue DataBrew is a visual data preparation tool that allows users, including non-coding users, to clean, normalize, and transform data using a no-code interface. It provides more than 250 pre-built transformations, making it easy to visually explore, preprocess, and clean data. Users can also schedule jobs to run these transformations periodically, fulfilling the need for automation.

## SageMaker Data Wrangler – End-to-End Data Prep for ML

**Exploration Tools**
- Histograms, scatter plots, correlation matrices

Line plots are helpful for identifying trends over time, which can be useful for analyzing relationships in time series data.

Histograms are an effective way to visualize the distribution of numerical data and are available in SageMaker Data Wrangler to help understand the spread of values in a dataset.

**Feature Engineering**
- Timestamp decomposition
- Categorical encoding, feature interactions

**Workflow Integration**
- Export data prep steps to `SageMaker Pipelines`

Data Wrangler can import data from multiple sources such as Amazon S3, Redshift, and SageMaker Feature Store, making it easy to preprocess diverse datasets. SageMaker Data Wrangler does not support direct integration with DynamoDB. AWS Glue is the right option to export data from DynamoDB to S3.

The Quick Model feature in Data Wrangler allows users to train a simple model quickly and visualize the feature importance. This helps in understanding which features contribute most to the model's predictions.

## SageMaker Processing Jobs – Custom Batch Pipelines

**Use Cases**
- Data preprocessing (e.g., image resize, cleaning)
- Postprocessing and evaluation
- Batch transformation or format conversion

**Features**
- Bring custom Docker containers (via ECR)
- Access data from S3, output back to S3
- Choose compute instance type (CPU/GPU)

**Integration**
- Works seamlessly within `SageMaker Pipelines`
- Ideal for **non-real-time, large-scale** workloads

Amazon SageMaker Processing Jobs provide a managed environment to preprocess data at scale, leveraging pre-built containers for tasks like normalization and encoding. This approach is scalable, integrated with SageMaker, and ideal for ML pipelines.

## Key Considerations for Data Ingestion Pipelines

- **Data Volume & Velocity**  
  Choose streaming services (Kinesis) for high-throughput, low-latency ingestion.  
  Use batch services (Glue, Batch) when latency is less critical and data volumes are large.

- **Data Transformation Needs**  
  Perform cleaning, enrichment, and feature engineering during ingestion to improve downstream model quality.  
  Visual tools (Data Wrangler) and code-based ETL (Glue, Processing Jobs) provide flexibility.

- **Fault Tolerance & Monitoring**  
  Implement retry policies, error handling, and incremental loading to ensure data consistency.  
  Use CloudWatch and AWS Glue job metrics to monitor pipeline health.

- **Scalability & Cost Optimization**  
  Automatically scale ingestion pipelines based on traffic patterns.  
  Optimize buffering and batching configurations to balance latency and cost.

- **Seamless Integration**  
  Leverage tight integration with storage (S3), compute (SageMaker), and analytics (Athena, Redshift) for end-to-end ML workflows.

# Merging and Processing Data from Multiple Sources

Merging and processing data from disparate sources is a cornerstone of machine learning dataset preparation. Combining structured and semi-structured datasets enhances feature richness, improves representation, and results in more accurate and generalizable ML models.

## Techniques for Data Merging

###  Python (Pandas)
- **Best For:** Small to medium-sized datasets; in-memory operations.
- **Usage Scenario:** Merging CSV files (e.g., sales transactions with customer demographics).
- **Capabilities:**
  - Joins: Inner, left, right, and outer joins via `merge()`.
  - Grouping and aggregation (`groupby`, `pivot_table`).
  - Fast prototyping and debugging.

### PySpark
- **Best For:** Large-scale, distributed datasets stored in the cloud.
- **Usage Scenario:** Merging S3-stored logs with relational data from Amazon RDS or Redshift.
- **Capabilities:**
  - Distributed joins and transformations.
  - Handles petabyte-scale datasets with resilience and fault tolerance.
  - Lazy evaluation for optimization.

## AWS Glue

AWS Glue is a fully managed ETL (Extract, Transform, Load) service that simplifies and automates merging, transformation, and movement of data across AWS services.

### Core Components

| Component        | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Crawlers**     | Automatically discover and classify schema from data sources.               |
| **Data Catalog** | Centralized metadata repository used across Glue, Athena, Redshift Spectrum. |
| **Glue Jobs**    | Spark-based ETL scripts (Python or Scala) that perform transformations.     |
| **Workflows**    | Define multi-step ETL pipelines using visual DAGs.                          |
| **Triggers**     | Schedule or event-based mechanisms to launch jobs automatically.            |
| **Monitoring**   | Track job performance, logs, errors through CloudWatch and built-in UI.     |

### Example Use Case: E-commerce Data Integration
A company wants to unify multiple data sources for ML training:
- **S3:** Order transactions
- **Amazon RDS:** Customer profiles
- **DynamoDB:** Real-time inventory changes

Using Glue, the company:
- Crawls all sources to generate schema.
- Performs ETL jobs to join datasets on customer ID and order ID.
- Outputs transformed, merged datasets to Amazon S3 or Redshift for downstream ML workflows.

### Key Features
- **In-Memory Processing:** Reduces I/O operations, accelerating iterative ML workflows.
- **Scalability:** Handles everything from a laptop to multi-node EMR clusters or Glue backends.
- **Fault Tolerance:** Resilient Distributed Dataset (RDD) model ensures task re-execution on failure.
- **Integration:** Works seamlessly with AWS services like S3, Redshift, RDS, and DynamoDB.