# 1.3 Data Ingestion on AWS

## Table of Contents
- [1.3.1 ETL and Data Processing Services](#131-etl-and-data-processing-services)
  - [1.3.1.1 AWS Glue – Scalable ETL Platform](#1311-aws-glue--scalable-etl-platform)
  - [1.3.1.2 AWS Glue DataBrew – Visual ETL](#1312-aws-glue-databrew--visual-etl)
  - [1.3.1.3 Amazon EMR – Managed Big Data Platform](#1313-amazon-emr--managed-big-data-platform)
- [1.3.2 SageMaker Data Preparation Services](#132-sagemaker-data-preparation-services)
  - [1.3.2.1 SageMaker Data Wrangler – End-to-End Data Prep](#1321-sagemaker-data-wrangler--end-to-end-data-prep)
  - [1.3.2.2 SageMaker Processing Jobs – Custom Batch Pipelines](#1322-sagemaker-processing-jobs--custom-batch-pipelines)
- [1.3.3 Data Ingestion Best Practices](#133-data-ingestion-best-practices)

---

## 1.3.1 ETL and Data Processing Services

### 1.3.1.1 AWS Glue – Scalable ETL Platform

#### Job Types and Execution
- **Spark Jobs**: Distributed processing for large-scale ETL operations
- **Python Shell**: Lightweight scripting for smaller datasets
- **Worker Types**: G.1X balances computational power and cost for moderate workloads

#### Automation and Triggers
- **Event-driven**: S3 events, EventBridge integration
- **Scheduled**: Time-based execution
- **On-demand**: Manual execution for testing

#### Key Features
- **Job Bookmarks**: Enable incremental loads for efficient processing
- **Push-down Predicates**: Improve query efficiency by filtering at source
- **Data Quality**: Built-in checks and profiling capabilities
- **Dynamic Frames**: Flexible processing of semi-structured data (financial transactions)

#### Advanced Capabilities

##### FindMatches
ML-based duplicate detection with minimal coding requirements:
- **Fuzzy Matching**: Identifies near-duplicates across key attributes
- **Scalable Processing**: Works with large S3 datasets
- **Flexible Logic**: Automatic similarity detection for names, addresses, emails

### 1.3.1.2 AWS Glue DataBrew – Visual ETL

#### Core Capabilities
- **No-Code Interface**: Drag-and-drop transformation and enrichment
- **Transformation Recipes**: Build reusable transformation workflows
- **250+ Pre-built Functions**: Handle missing values, outliers, type conversions
- **Text Processing**: Tokenization, stemming, and other NLP operations

#### Data Profiling Features
- **Statistical Analysis**: Data distributions, value ranges
- **Schema Detection**: Automatic schema inference
- **Quality Assessment**: Data quality scoring and recommendations

#### Integration and Automation
- **SageMaker Export**: Output in ML formats (CSV, Parquet, JSON)
- **Scheduled Jobs**: Automated execution for periodic data preparation
- **Multi-user Access**: Enables non-coding users to prepare data

### 1.3.1.3 Amazon EMR – Managed Big Data Platform

####  Node Types
- **Primary Node (Master Node)**: 
  - Manages the cluster and coordinates data distribution
  - Tracks job status and monitors cluster health
  - Single node that coordinates all activities

- **Core Nodes**:
  - Run data processing tasks and store data in HDFS
  - Execute TaskTracker and DataNode processes
  - Handle both compute and storage responsibilities

- **Task Nodes** (Optional):
  - Run only TaskTracker processes for job execution
  - Can be added/removed dynamically without data loss
  - Ideal for temporary workload scaling using Spot instances

#### Key Features for Data Ingestion
- **Multiple Processing Frameworks**: Spark, Hadoop, Hive, Presto, Flink
- **Auto Scaling**: Automatically add/remove task nodes based on workload
- **Spot Instance Support**: Cost-effective processing using EC2 Spot instances

#### Integration Capabilities
- **S3 as Storage**: Use S3 as primary data lake storage (EMRFS)
- **Data Sources**: Direct connectivity to RDS, DynamoDB, Redshift
- **Streaming**: Real-time processing with Spark Streaming or Flink
- **AWS Glue Integration**: Use Glue Data Catalog for metadata management

## 1.3.2 SageMaker Data Preparation Services

### 1.3.2.1 SageMaker Data Wrangler – End-to-End Data Prep

#### Data Exploration Tools
- **Visualization**: Histograms, scatter plots, correlation matrices
- **Time Series**: Line plots for identifying trends over time
- **Distribution Analysis**: Understand data spread and patterns

#### Feature Engineering Capabilities
- **Timestamp Decomposition**: Extract date/time components
- **Categorical Encoding**: Handle categorical variables for ML
- **Feature Interactions**: Create derived features from existing data

#### Workflow Integration
- **SageMaker Pipelines**: Export data prep steps for automation
- **Multi-source Import**: S3, Redshift, SageMaker Feature Store
- **Quick Model**: Train simple models to visualize feature importance

#### Key Limitations
- **No DynamoDB Integration**: Use AWS Glue to export DynamoDB to S3 first

### 1.3.2.2 SageMaker Processing Jobs – Custom Batch Pipelines

#### Use Cases
- **Data Preprocessing**: Image resize, data cleaning, normalization
- **Postprocessing**: Model evaluation, result analysis
- **Batch Transformation**: Format conversion, feature extraction

#### Key Features
- **Custom Containers**: Bring Docker containers via ECR
- **Flexible Compute**: Choose CPU/GPU instance types based on workload
- **S3 Integration**: Direct access to S3 for input and output
- **Scalability**: Managed environment for large-scale preprocessing

#### Integration Benefits
- **SageMaker Pipelines**: Seamless integration for ML workflows
- **Non-real-time Processing**: Ideal for large-scale batch workloads
- **Pre-built Containers**: Available for common tasks like normalization and encoding

## 1.3.3 Data Ingestion Best Practices

### Performance and Scalability
- **Data Volume & Velocity**: Choose streaming services (Kinesis) for high-throughput, low-latency ingestion; use batch services (Glue, Batch) when latency is less critical
- **Scalability**: Automatically scale ingestion pipelines based on traffic patterns
- **Cost Optimization**: Optimize buffering and batching configurations to balance latency and cost

### Data Quality and Transformation
- **Transformation Needs**: Perform cleaning, enrichment, and feature engineering during ingestion to improve downstream model quality
- **Tool Selection**: Visual tools (Data Wrangler) and code-based ETL (Glue, Processing Jobs) provide flexibility
- **Quality Assurance**: Implement data validation and profiling throughout the pipeline

### Reliability and Monitoring
- **Fault Tolerance**: Implement retry policies, error handling, and incremental loading to ensure data consistency
- **Pipeline Monitoring**: Use CloudWatch and AWS Glue job metrics to monitor pipeline health
- **Error Management**: Set up alerts and automated recovery procedures

### Integration Strategy
- **End-to-End Workflows**: Leverage tight integration with storage (S3), compute (SageMaker), and analytics (Athena, Redshift)
- **Service Coordination**: Design pipelines that work seamlessly across multiple AWS services
- **Version Control**: Maintain data lineage and transformation history