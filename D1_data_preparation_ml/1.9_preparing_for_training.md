# 1.9 Preparing Data for Model Training

- [1.9.1 Core Techniques for Data Preparation](#191-core-techniques-for-data-preparation)
- [1.9.2 Efficient Storage for Training Data](#192-efficient-storage-for-training-data)

---

### 1.9.1 Core Techniques for Data Preparation

Effective data preparation is essential for building accurate and fair machine learning models. The following are the most important techniques:

**Dataset Splitting**
Divide your data into:
- **Training set** (70–80%)
- **Validation set** (10–20%)
- **Test set** (10%)

This ensures you can train, tune, and evaluate your model without data leakage.

**Shuffling**
Randomize your dataset before splitting to remove unwanted ordering or biases that could affect model learning.

**Data Augmentation**
Generate synthetic data to improve model generalization, especially in computer vision (e.g., flips, rotations, color shifts) or NLP (e.g., synonym replacement, back-translation).

**Feature Scaling and Normalization**
Ensure all features are on a consistent scale. Use tools like **MinMaxScaler** or **RobustScaler** in **SageMaker Data Wrangler** to standardize your data, which helps many ML algorithms converge faster and perform better.

---
### 1.9.2 Efficient Storage for Training Data

Choosing the right storage solution is key for scalable and efficient model training:

**Amazon Elastic File System (EFS):**
- Best for real-time training across multiple EC2 instances
- Scalable, shared, and easy to mount on many servers

**Amazon FSx for Lustre:**
- Designed for high-performance computing (HPC) workloads
- Delivers sub-millisecond latency and high throughput


## Summary

Data preparation is critical for model accuracy and fairness:
- Always shuffle and split datasets properly
- Use data augmentation to improve model generalization
- Apply scaling techniques for consistency
- Store data in scalable, low-latency systems like EFS and FSx based on your use case
