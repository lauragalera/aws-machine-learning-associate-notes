# 1.1 Data Formats

- [1.1.1 Validated vs. Non-validated Formats](#111-validated-vs-non-validated-formats)
  - [Validated Formats](#validated-formats)
  - [Non-validated Formats](#non-validated-formats)
- [1.1.2 Common Data Formats Overview](#112-common-data-formats-overview)
  - [Key Format Categories](#key-format-categories)
- [1.1.3 Format Selection Guidelines](#113-format-selection-guidelines)
  - [By Data Structure](#by-data-structure)
  - [By Performance Requirements](#by-performance-requirements)
  - [By Use Case](#by-use-case)
  - [Tool Compatibility Considerations](#tool-compatibility-considerations)
  - [Parquet for SageMaker Canvas](#parquet-for-sagemaker-canvas)

---

## 1.1.1 Validated vs. Non-validated Formats

### Validated Formats
Structured data that has been cleaned and validated to ensure it conforms to expected formats.

**Key Components:**
- Schema validation
- Data type enforcement
- Range/consistency checks
- Automated validation tools: **Great Expectations**, **AWS Glue DataBrew**

### Non-validated Formats
Raw, unprocessed data with potential issues in structure and integrity.

**Common Risks:**
- Inconsistent schema
- Incorrect data types or values
- ML pipeline instability
- Downstream processing failures

**Examples:**
- Plain text logs
- Unstructured `JSON`
- Raw system outputs

## 1.1.2 Common Data Formats Overview


| Format       | Description                                                  | Use Cases                            | AWS Services                                          |
|--------------|--------------------------------------------------------------|--------------------------------------|-------------------------------------------------------|
| **CSV**      | Plain-text tabular data, comma or delimiter-separated        | Spreadsheets, exports, tabular data  | S3, Glue, Athena, Redshift, Data Wrangler             |
| **JSON**     | Semi-structured, hierarchical key-value format               | Web APIs, configs, metadata          | S3, Lambda, API Gateway, DynamoDB, Data Wrangler      |
| **Parquet**  | Compressed columnar format optimized for analytics           | Data lakes, fast querying            | S3, Glue, Athena, Redshift Spectrum, Data Wrangler    |
| **ORC**      | Efficient columnar format, ideal for Hive/Hadoop ecosystems  | Hive, big data lakes                 | S3, Glue, EMR, Hive                                   |
| **Avro**     | Row-based, schema-aware binary format                        | Event streaming, Kafka, schema evolution | S3, Kinesis, Glue, Schema Registry               |
| **RecordIO** | Binary format optimized for deep learning inputs             | Large-scale DL datasets              | MXNet, SageMaker DL Containers                        |
| **Protobuf** | Efficient cross-platform binary serialization                | Real-time comms, inter-service APIs  | S3, Kinesis, API Gateway                              |

#### Key Format Categories
- **Structured Data**: `CSV` - Fixed schema, tabular format
- **Columnar with Schema Evolution**: `Parquet`, `ORC` - Structured but supports schema changes
- **Semi-structured Data**: `JSON` - Flexible schema, nested objects
- **Streaming Data**: `Avro` - Schema evolution support, row-based
- **ML-Optimized**: `RecordIO`, `TFRecord` - Specialized for deep learning

## 1.1.3 Format Selection Guidelines

### By Data Structure:
- **Fixed Schema**: `CSV` - Rigid tabular structure
- **Columnar with Schema Evolution**: `Parquet`, `ORC` - Structured but extensible
- **Semi-structured**: `JSON` - Flexible, hierarchical data
- **Full Schema Evolution**: `Avro` - Forward/backward compatibility
- **ML-specific**: `RecordIO`, `TFRecord` - Optimized for training

### By Performance Requirements:
- **Compression**: `Parquet`, `ORC`, `Avro` support `Snappy`, `Gzip`
- **Query Performance**: Columnar formats enable projection & predicate pushdown
- **Parallel Processing**: `Parquet` and `ORC` support parallel read/write

### By Use Case:
- **Data Lakes**: `Parquet`, `ORC` - Analytics-optimized
- **Real-time Streaming**: `Avro`, `JSON` - Schema flexibility
- **Deep Learning**: `RecordIO`, `TFRecord` - Training efficiency
- **API Integration**: `JSON`, `Protobuf` - Cross-platform compatibility

### Tool Compatibility Considerations:
- Ensure format compatibility with target services (`SageMaker`, `Spark`, `Athena`)
- Consider data pipeline tools and their supported formats
- Evaluate schema evolution requirements for long-term data management

#### Parquet for SageMaker Canvas
Apache Parquet is specifically designed for large-scale data processing, making it ideal for machine learning workflows with complex datasets. Its columnar format reduces the amount of data that needs to be processed by enabling efficient selection of relevant columns, thereby minimizing data processing time. Parquet also supports compression and parallel processing, which further enhances its performance when handling large datasets like transactional records. This makes it the most suitable file format for **importing data into SageMaker Canvas efficiently**.