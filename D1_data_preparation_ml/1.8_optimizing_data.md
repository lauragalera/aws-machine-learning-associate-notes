# Data Preprocessing for Enhanced Model Performance

## Common Problems in ML Models

### Overfitting
When a model performs well on **training data** but poorly on **new/unseen data**.

### Prediction Bias
Occurs when a model makes **biased predictions** due to **imbalanced** or **poorly handled data**.

## Key Techniques to Reduce Bias & Overfitting

### 1. **Cross-Validation**
- Splits dataset into **multiple folds** (e.g., k-fold).
- Ensures model performs well across different data subsets.

### 2. **Feature Engineering**
- Create **new derived features** to improve model performance.

### 3. **Regularization**
- Applies penalties to reduce model complexity:
  - **L1 (Lasso)**: Pushes coefficients to zero (feature selection).
  - **L2 (Ridge)**: Penalizes large weights smoothly.

### 4. **Handling Imbalanced Datasets**
- Use:
  - **Oversampling** (e.g., SMOTE),
  - **Undersampling**, or
  - **Class weighting**.

### 5. **Temperature Scaling**
- **Post-training** calibration technique.
- Adjusts prediction probabilities to better reflect real-world uncertainty.

## Optimizing Training Data via Transformation & Augmentation

### Data Transformations
- Improve data quality and model learning:
  - **Normalization** (e.g., MinMaxScaler)
  - **Standardization** (e.g., z-score)
  - **One-Hot Encoding / Label Encoding**

### Data Augmentation
- Increases data variety **without collecting new data**.

## Summary

- Use **cross-validation** and **regularization** to combat overfitting.
- Apply **feature engineering** and **data transformations** to enrich inputs.
- Fight **bias** with proper dataset balancing and **temperature scaling**.
- For robust generalization, integrate **augmentation techniques** across domains like weather, health, and fraud detection.

# Data Loading and Configuration for Model Training

## Efficient Data Loading Strategies for ML

Efficient loading ensures **faster training** and avoids **resource issues** like memory bottlenecks.

### 1. **Batch Data Loading**
- Splits dataset into **mini-batches** during training.
- Reduces memory usage and allows smooth processing.

### 2. **Pre-fetching and Data Pipelining**
- **Prepares next data batch** while current one is processed.
- Reduces **training idle time**.

### 3. **Data Caching**
- Stores frequently accessed data **in-memory** (especially from remote sources like **Amazon S3**).
- Reduces I/O operations and boosts performance.

### 4. **Sharding**
- Divides large datasets into **independent shards** for parallel training in distributed systems.

## Using Amazon FSx and Amazon EFS for Training Data Storage

### Amazon FSx for Lustre
- Ideal for **high-performance computing (HPC)** and **ML workloads**.
- Seamless integration with **Amazon S3** for massive data access.

### Amazon EFS (Elastic File System)
- Scales automatically and supports **parallel EC2 access**.
- Replicates data across **multiple Availability Zones** (AZs).

## Summary

| Technique        | Benefit                              | 
|------------------|---------------------------------------|
| Batch Loading     | Saves memory                         |
| Pre-fetching      | Reduces idle training time           |
| Caching           | Faster repeated access               |
| Sharding          | Enables parallel data processing     |
| FSx for Lustre    | Fast + HPC workloads + S3 integration|
| Amazon EFS        | Auto-scaling + shared EC2 access     | 

