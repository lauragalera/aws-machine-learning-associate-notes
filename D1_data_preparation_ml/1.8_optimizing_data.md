# Data Preprocessing for Enhanced Model Performance

## Common Problems in ML Models

### Overfitting
When a model performs well on **training data** but poorly on **new/unseen data**.

How to detect it? A significant gap between training and validation loss typically indicates overfitting, as the model fits the training data too closely but fails to generalize.

### Prediction Bias
Occurs when a model makes **biased predictions** due to **imbalanced** or **poorly handled data**.

## Key Techniques to Reduce Bias & Overfitting

### 1. **Cross-Validation**
- Splits dataset into **multiple folds** (e.g., k-fold).
- Ensures model performs well across different data subsets.

### 2. **Feature Engineering**
- Create **new derived features** to improve model performance.

### 3. **Regularization**
- Applies penalties to reduce model complexity:
  - **L1 (Lasso)**: Pushes coefficients to zero (feature selection).
  - **L2 (Ridge)**: Penalizes large weights smoothly.

L1 regularization (Lasso) also reduces model complexity and can help with variance by eliminating features. However, it introduces sparsity and may discard useful correlated features, making L2 a more suitable choice when all features contribute value, as is often the case in churn prediction.

### 4. **Handling Imbalanced Datasets**
- Use:
  - **Oversampling** (e.g., SMOTE),
  - **Undersampling**, or
  - **Class weighting**.

### 5. **Temperature Scaling**
- **Post-training** calibration technique.
- Adjusts prediction probabilities to better reflect real-world uncertainty.

## Optimizing Training Data via Transformation & Augmentation

### Data Transformations
- Improve data quality and model learning:
  - **Normalization** (e.g., MinMaxScaler)
  - **Standardization** (e.g., z-score)
  - **One-Hot Encoding / Label Encoding**

### Data Augmentation
- Increases data variety **without collecting new data**.

## Summary

- Use **cross-validation** and **regularization** to combat overfitting.
- Apply **feature engineering** and **data transformations** to enrich inputs.
- Fight **bias** with proper dataset balancing and **temperature scaling**.
- For robust generalization, integrate **augmentation techniques** across domains like weather, health, and fraud detection.

# Data Loading and Configuration for Model Training

## Efficient Data Loading Strategies for ML

Efficient loading ensures **faster training** and avoids **resource issues** like memory bottlenecks.

### 1. **Batch Data Loading**
- Splits dataset into **mini-batches** during training.
- Reduces memory usage and allows smooth processing.

### 2. **Pre-fetching and Data Pipelining**
- **Prepares next data batch** while current one is processed.
- Reduces **training idle time**.

### 3. **Data Caching**
- Stores frequently accessed data **in-memory** (especially from remote sources like **Amazon S3**).
- Reduces I/O operations and boosts performance.

### 4. **Sharding**
- Divides large datasets into **independent shards** for parallel training in distributed systems.

## Using Amazon FSx and Amazon EFS for Training Data Storage

In Amazon SageMaker, File Mode and Pipe Mode describe how your training job accesses the input data stored in Amazon S3.

**File Mode** means that before your training job actually starts, SageMaker copies the entire dataset from S3 to the local storage volume attached to the training instance. This means all your data is downloaded and saved locally first, and then your training script reads from those local files. This approach is straightforward and works well for small or medium-sized datasets or when your training process needs to randomly access the data.

**Pipe Mode** works differently. Instead of downloading the whole dataset upfront, SageMaker streams the data directly from S3 into the training container through a streaming interface called a named pipe. This allows your training job to start immediately and read data as it arrives, which is especially useful for very large datasets. This is a better options than Lustre + Fast File because it doesn't require provisioning and managing a separate file system.

### Amazon FSx for Lustre

For datasets that are too large for File Mode, contain many small files that are hard to serialize, or require random read access patterns, Amazon FSx for Lustre offers a high-performance solution. SageMaker mounts the FSx for Lustre file system directly to the training instanceâ€™s file system before starting the training script. The mount operation is fast and does not depend on the dataset size.

FSx for Lustre can scale to hundreds of gigabytes per second of throughput and millions of IOPS with very low latency, making it ideal for workloads with many small files or demanding performance requirements.

- Ideal for **high-performance computing (HPC)** and **ML workloads**.
- Seamless integration with **Amazon S3** for massive data access.

## Fast File Mode for Large Video Files in S3

Fast File Mode is a performance optimization feature in Amazon SageMaker designed to accelerate file system operations when using managed network file systems such as Amazon FSx for Lustre or Amazon EFS as the data source for training jobs. It reduces the overhead associated with metadata operations and file handling, providing faster file access and lower latency during training.

While FSx for Lustre excels at handling many small files with low latency, it is not the optimal solution for providing low-latency access to many large video files stored in Amazon S3. For such use cases, Fast File Mode (a feature related to AWS Transfer Family) can be employed to enable efficient on-demand streaming of these large files directly from S3.

You should combine FSx for Lustre for many small image files, and Fast File Mode to efficiently stream large video files stored in S3.

### Amazon EFS (Elastic File System)
- Scales automatically and supports **parallel EC2 access**.
- Replicates data across **multiple Availability Zones** (AZs).

## Summary

| Technique        | Benefit                              | 
|------------------|---------------------------------------|
| Batch Loading     | Saves memory                         |
| Pre-fetching      | Reduces idle training time           |
| Caching           | Faster repeated access               |
| Sharding          | Enables parallel data processing     |
| FSx for Lustre    | Fast + HPC workloads + S3 integration|
| Amazon EFS        | Auto-scaling + shared EC2 access     | 

