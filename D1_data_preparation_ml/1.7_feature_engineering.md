

# 1.7 Feature Engineering and Tools

- [1.7.1 Feature Engineering Techniques](#171-feature-engineering-techniques)
  - [Principal Component Analysis (PCA)](#principal-component-analysis-pca)
  - [Data Scaling and Standardization](#data-scaling-and-standardization)
  - [Feature Splitting, Binning, and Log Transformation](#feature-splitting-binning-and-log-transformation)
  - [Encoding Techniques for ML](#encoding-techniques-for-ml)
  - [Tokenization for Text Data](#tokenization-for-text-data)
- [1.7.2 Tools for Data Exploration & Feature Engineering](#172-tools-for-data-exploration--feature-engineering)
  - [SageMaker Data Wrangler](#sagemaker-data-wrangler)
  - [AWS Glue DataBrew](#aws-glue-databrew)
- [1.7.3 Transforming Streaming Data](#173-transforming-streaming-data)
  - [Real-time Processing with AWS Lambda](#real-time-processing-with-aws-lambda)
  - [Spark-based Streaming in Amazon EMR](#spark-based-streaming-in-amazon-emr)
- [Summary Table](#summary-table)

---

## 1.7.1 Feature Engineering Techniques

Feature engineering transforms raw data into meaningful inputs for machine learning models, improving accuracy and performance.

### Principal Component Analysis (PCA)

PCA is an **unsupervised technique** for dimensionality reduction that transforms data into a lower-dimensional space without using labels. It reduces dimensionality by transforming data into principal components that capture the most variance, effectively filtering out noise spread across less important components.

### Data Scaling and Standardization

Scaling and standardization prevent large-magnitude features (e.g., price) from dominating smaller ones (e.g., number of bedrooms).

- **MinMaxScaler**: Scales features to a 0–1 range.
- **StandardScaler**: Normalizes data with a mean of 0 and standard deviation of 1.

### Feature Splitting, Binning, and Log Transformation

- **Feature Binning**: Groups numeric values into ranges (e.g., purchase quantity: 1–3, 4–6, etc.)
- **Log Transformation**: Applies logarithmic scale to reduce skewness in highly variable features.

### Encoding Techniques for ML

Encoding transforms categorical/text data into numerical form for use in ML models.

| Technique        | Description | Example |
|------------------|-------------|---------|
| **One-hot Encoding** | Creates binary vectors for categories | "Bad", "Good", "Excellent" → `[1, 0, 0]` |
| **Binary Encoding**  | Assigns numerical labels, then converts to binary | "Groceries" = 3 → Binary: `011` |
| **Label Encoding**   | Assigns integer values based on category order | "Silver", "Gold", "Platinum" → 0, 1, 2 |

### Tokenization for Text Data

Tokenization splits text into smaller units: **words, subwords**, or **characters**.


**Example**  

> "Machine learning is powerful!"

| Tokenization Type | Output                                 | Use Case                  |
|-------------------|----------------------------------------|---------------------------|
| Word              | ["Machine", "learning", "is", "powerful!"] | Simpler NLP tasks         |
| Subword           | ["Mach", "ine", "learn", "ing", "is", "power", "ful", "!"] | Robust to unknown words |
| Character         | ["M", "a", "c", "h", "i", "n", "e", ...]         | Language-agnostic models |
---
## 1.7.2 Tools for Data Exploration & Feature Engineering

### SageMaker Data Wrangler
- **No-code** environment for cleaning, transforming, and visualizing data.
- Supports outlier removal, imputation, feature creation, and balancing for classification.

### AWS Glue DataBrew

**AWS Glue DataBrew** is a visual, no-code tool for data cleaning and transformation. It lets you profile, clean, and prepare data with a point-and-click interface, making it accessible to users who don't want to write code. DataBrew is great for:
- Exploring and profiling data visually
- Applying 250+ built-in transformations (e.g., deduplication, normalization, outlier removal)
- Generating transformation recipes that can be reused in production

| Tool       | Use Case                                 | Interface    |
|------------|------------------------------------------|--------------|
| **AWS Glue** | Serverless ETL for large-scale pipelines | Script-based |
| **DataBrew** | Visual data cleaning and transformation  | No-code GUI  |
---
## 1.7.3 Transforming Streaming Data

### Real-time Processing with AWS Lambda
- Serverless compute that reacts to events in real time
- Integrates with Amazon Kinesis Data Streams and DynamoDB Streams

> **Note:** Amazon Kinesis Data Streams is designed for real-time data ingestion and streaming. While it can process large volumes of sensor data in real time, it does not include built-in anomaly detection capabilities or support data aggregation and preprocessing for machine learning workflows.

**Example:**
A fintech company in Quezon City monitors transactions and triggers fraud alerts via Lambda when unusual patterns occur.

### Spark-based Streaming in Amazon EMR
- Leverages Apache Spark on EMR for real-time stream processing at scale
- Great for large, fast-changing datasets

**Example:**
A media company uses EMR with Spark to monitor **live viewer engagement** and update dashboards in near-real-time.

## Summary Table

| Task                      | Tool/Technique                                      |
|---------------------------|-----------------------------------------------------|
| Scale numeric data        | MinMaxScaler, StandardScaler                        |
| Handle skewed distributions | Binning, Log Transform                            |
| Encode categories         | One-hot, Label, Binary Encoding                    |
| Tokenize text             | Word, Subword, Character Tokenization               |
| Visual data prep          | SageMaker Data Wrangler, AWS Glue DataBrew         |
| Large ETL pipelines       | AWS Glue                                            |
| Streaming transformation  | AWS Lambda, Amazon EMR with Spark                  |
